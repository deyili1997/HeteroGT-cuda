{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105bbf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 1.13.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from model import SetGNN \n",
    "import pickle\n",
    "from tokenizer import EHRTokenizer\n",
    "from dataset import FinetuneHGDataset, batcher_SetGNN_finetune\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from train import PHENO_ORDER, train_with_early_stopping\n",
    "from set_seed import set_random_seed\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb757cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89793d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset\": \"MIMIC-IV\", \n",
    "    \"task\": \"next_diag_12m\",  # options: death, stay, readmission, next_diag_6m, next_diag_12m\n",
    "    \"special_tokens\":[\"[PAD]\", \"[CLS]\"],\n",
    "    \"predicted_token_type\": [\"diag\", \"med\", \"lab\", \"pro\"],\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 1e-3,\n",
    "    \"epochs\": 500,\n",
    "    \"model_name\": \"HG\",\n",
    "    \"early_stop_patience\": 10,\n",
    "    # model hyperparameters\n",
    "    \"level\": \"patient\",  # \"visit\" or \"patient\"\n",
    "    \"hg_all_num_layers\": 3,\n",
    "    \"hg_use_type_embed\": True,\n",
    "    \"MLP_num_layers\": 2,\n",
    "    \"hg_aggregate\": \"mean\",\n",
    "    \"hg_dropout\": 0.0,\n",
    "    \"normtype\": \"all_one\",\n",
    "    \"add_self_loop\": True,\n",
    "    \"hg_normalization\": \"ln\",\n",
    "    \"hg_hidden_size\": 128,\n",
    "    \"PMA\": True,\n",
    "    \"hg_num_heads\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32fba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path = f\"/home/lideyi/HeteroGT-cuda/data_process/{args['dataset']}-processed/mimic.pkl\"\n",
    "\n",
    "if args[\"task\"] == \"next_diag_6m\":\n",
    "    finetune_data_path = f\"/home/lideyi/HeteroGT-cuda/data_process/{args['dataset']}-processed/mimic_nextdiag_6m.pkl\"\n",
    "elif args[\"task\"] == \"next_diag_12m\":\n",
    "    finetune_data_path = f\"/home/lideyi/HeteroGT-cuda/data_process/{args['dataset']}-processed/mimic_nextdiag_12m.pkl\"\n",
    "else:\n",
    "    finetune_data_path = f\"/home/lideyi/HeteroGT-cuda/data_process/{args['dataset']}-processed/mimic_downstream.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab08e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max admissions per patient: 8\n"
     ]
    }
   ],
   "source": [
    "ehr_full_data = pickle.load(open(full_data_path, 'rb'))\n",
    "diag_sentences = ehr_full_data[\"ICD9_CODE\"].values.tolist()\n",
    "med_sentences = ehr_full_data[\"NDC\"].values.tolist()\n",
    "lab_sentences = ehr_full_data[\"LAB_TEST\"].values.tolist()\n",
    "pro_sentences = ehr_full_data[\"PRO_CODE\"].values.tolist()\n",
    "age_gender_sentences = [\"[PAD]\"] + [str(c) + \"_\" + gender \\\n",
    "    for c in set(ehr_full_data[\"AGE\"].values.tolist()) for gender in [\"M\", \"F\"]] # PAD token special for age_gender vocabulary\n",
    "max_admissions = ehr_full_data.groupby(\"SUBJECT_ID\")[\"HADM_ID\"].nunique().max()\n",
    "args[\"max_adm_len\"] = max_admissions\n",
    "print(f\"Max admissions per patient: {max_admissions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42864fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age and gender vocabulary size: 41\n",
      "Global vocabulary size: 4207\n",
      "Label vocabulary size: 18\n"
     ]
    }
   ],
   "source": [
    "tokenizer = EHRTokenizer(age_gender_sentences, diag_sentences, med_sentences, lab_sentences, \n",
    "                         pro_sentences, special_tokens=args[\"special_tokens\"])\n",
    "args[\"age_gender_vocab_size\"] = tokenizer.token_number(\"age_gender\")\n",
    "args[\"global_vocab_size\"] = len(tokenizer.vocab.id2word)\n",
    "args[\"label_vocab_size\"] = len(PHENO_ORDER)\n",
    "print(f\"Age and gender vocabulary size: {args['age_gender_vocab_size']}\")\n",
    "print(f\"Global vocabulary size: {args['global_vocab_size']}\")\n",
    "print(f\"Label vocabulary size: {args['label_vocab_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ff21ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6850 5046 5062\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = pickle.load(open(finetune_data_path, 'rb'))\n",
    "\n",
    "# output: input_ids (a patient has multiple visits), labels\n",
    "train_dataset = FinetuneHGDataset(train_data, tokenizer, token_type=args[\"predicted_token_type\"], task=args[\"task\"], level=args[\"level\"])\n",
    "val_dataset = FinetuneHGDataset(val_data, tokenizer, token_type=args[\"predicted_token_type\"], task=args[\"task\"], level=args[\"level\"])\n",
    "test_dataset = FinetuneHGDataset(test_data, tokenizer, token_type=args[\"predicted_token_type\"], task=args[\"task\"], level=args[\"level\"])\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42afdd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857 876\n"
     ]
    }
   ],
   "source": [
    "long_adm_seq_crite = 3\n",
    "val_long_seq_idx, test_long_seq_idx = [], []\n",
    "for i in range(len(val_dataset)):\n",
    "    hadm_id = list(val_dataset.records.keys())[i]\n",
    "    num_adms = len(val_dataset.records[hadm_id])\n",
    "    if num_adms >= long_adm_seq_crite:\n",
    "        val_long_seq_idx.append(i)\n",
    "for i in range(len(test_dataset)):\n",
    "    hadm_id = list(test_dataset.records.keys())[i]\n",
    "    num_adms = len(test_dataset.records[hadm_id])\n",
    "    if num_adms >= long_adm_seq_crite:\n",
    "        test_long_seq_idx.append(i)\n",
    "print(len(val_long_seq_idx), len(test_long_seq_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0769b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_full_graph = True\n",
    "train_batch_size = len(train_dataset) if use_full_graph else args[\"batch_size\"]\n",
    "val_batch_size = len(val_dataset) if use_full_graph else args[\"batch_size\"]\n",
    "test_batch_size = len(test_dataset) if use_full_graph else args[\"batch_size\"]\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=batcher_SetGNN_finetune(device = device), batch_size = train_batch_size, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=batcher_SetGNN_finetune(device = device), batch_size = val_batch_size, shuffle = False)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=batcher_SetGNN_finetune(device = device), batch_size = test_batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a62432",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"task\"] in [\"death\", \"stay\", \"readmission\"]:\n",
    "    eval_metric = \"prauc\"\n",
    "    task_type = \"binary\"\n",
    "    loss_fn = F.binary_cross_entropy_with_logits\n",
    "else:\n",
    "    eval_metric = \"prauc\"\n",
    "    task_type = \"l2r\"\n",
    "    loss_fn = lambda x, y: F.binary_cross_entropy_with_logits(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33afa400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2746317213, 1181241943, 958682846, 3163119785, 1812140441, 127978094, 939042955, 2340505846, 946785248, 2530876844, 3460967357, 2998485882, 1461364854, 667779376, 1445662585]\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "seeds = [random.randint(0, 2**32 - 1) for _ in range(15)]\n",
    "print(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af0347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random seed set to 2746317213\n",
      "Training with seed: 2746317213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Average Loss: 0.6892\n",
      "Validation: {'precision': 0.11760698198360703, 'recall': 0.18848865371940005, 'f1': 0.09564478904182769, 'auc': 0.5356592983214251, 'prauc': 0.19357521604115235}\n",
      "Test:       {'precision': 0.10947074532390982, 'recall': 0.18784223596121932, 'f1': 0.09331952433306151, 'auc': 0.5369945186671163, 'prauc': 0.19617338036109344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Average Loss: 0.6796\n",
      "Validation: {'precision': 0.14072426929849532, 'recall': 0.10796984170098531, 'f1': 0.07465882485041112, 'auc': 0.5505306341708036, 'prauc': 0.20441414682307515}\n",
      "Test:       {'precision': 0.1192145411472168, 'recall': 0.10519121431265692, 'f1': 0.07385940242854719, 'auc': 0.5540175667618117, 'prauc': 0.21105374312857006}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Average Loss: 0.6705\n",
      "Validation: {'precision': 0.10685244626077331, 'recall': 0.0656404309739096, 'f1': 0.06287167201861112, 'auc': 0.5547758289838539, 'prauc': 0.21095136466634362}\n",
      "Test:       {'precision': 0.1160454594130049, 'recall': 0.06734835627707456, 'f1': 0.0648236090598583, 'auc': 0.556981580526336, 'prauc': 0.21864330764332526}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Average Loss: 0.6602\n",
      "Validation: {'precision': 0.08819728465109786, 'recall': 0.04469012085583524, 'f1': 0.05256911058362132, 'auc': 0.5548787048136268, 'prauc': 0.21515641033044627}\n",
      "Test:       {'precision': 0.09656243689664008, 'recall': 0.048210928080093945, 'f1': 0.05627590577863902, 'auc': 0.5564413564442686, 'prauc': 0.22246346971533132}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Average Loss: 0.6483\n",
      "Validation: {'precision': 0.08850183751661583, 'recall': 0.022764975210427158, 'f1': 0.03326311769935336, 'auc': 0.5534051705869376, 'prauc': 0.21622898492682746}\n",
      "Test:       {'precision': 0.1607682410385047, 'recall': 0.027913289325287416, 'f1': 0.04086816387017185, 'auc': 0.5543100795883658, 'prauc': 0.22389201827862357}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Average Loss: 0.6342\n",
      "Validation: {'precision': 0.08258198280453473, 'recall': 0.01423570152569198, 'f1': 0.01833420150354513, 'auc': 0.5492809576688805, 'prauc': 0.21329049433165156}\n",
      "Test:       {'precision': 0.12259300229432973, 'recall': 0.016037447467044965, 'f1': 0.021305108844034314, 'auc': 0.5508362220297583, 'prauc': 0.2206106603791538}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Average Loss: 0.6184\n",
      "Validation: {'precision': 0.02374301675977654, 'recall': 0.014617368873602753, 'f1': 0.018094731240021287, 'auc': 0.545269350392295, 'prauc': 0.20889592838993687}\n",
      "Test:       {'precision': 0.047214569941842664, 'recall': 0.0164780479561835, 'f1': 0.020976387306656456, 'auc': 0.5467526377806794, 'prauc': 0.21611759559718563}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Average Loss: 0.6002\n",
      "Validation: {'precision': 0.024205748865355523, 'recall': 0.013757523645743765, 'f1': 0.017543859649122806, 'auc': 0.5416244784119602, 'prauc': 0.20529171474114213}\n",
      "Test:       {'precision': 0.02992075044476791, 'recall': 0.016223800754187494, 'f1': 0.02103946320937109, 'auc': 0.5430444522176298, 'prauc': 0.2116226431223013}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Average Loss: 0.5800\n",
      "Validation: {'precision': 0.027583527583527584, 'recall': 0.010174835196331326, 'f1': 0.014865996649916249, 'auc': 0.5385403607333139, 'prauc': 0.20141779962237652}\n",
      "Test:       {'precision': 0.03260357815442561, 'recall': 0.012145926510567395, 'f1': 0.01769854961344323, 'auc': 0.5398805592614001, 'prauc': 0.20677422488608288}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Average Loss: 0.5589\n",
      "Validation: {'precision': 0.03835978835978836, 'recall': 0.002770612400878953, 'f1': 0.00516795865633075, 'auc': 0.535692720074612, 'prauc': 0.19656356637300362}\n",
      "Test:       {'precision': 0.042105263157894736, 'recall': 0.0031570639305445935, 'f1': 0.005873715124816447, 'auc': 0.5369100575561626, 'prauc': 0.2011282973130574}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Average Loss: 0.5364\n",
      "Validation: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5344937242186473, 'prauc': 0.19361483994001494}\n",
      "Test:       {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5354573199762948, 'prauc': 0.19767854108448202}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Average Loss: 0.5142\n",
      "Validation: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5329369977067552, 'prauc': 0.19070120582795508}\n",
      "Test:       {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5337099335664335, 'prauc': 0.19408466192418852}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Running inference: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Average Loss: 0.4934\n",
      "Validation: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5321989118003434, 'prauc': 0.187111762489665}\n",
      "Test:       {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5326847518505664, 'prauc': 0.1896349611198469}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "final_metrics, final_long_seq_metrics = [], []\n",
    "\n",
    "for seed in seeds:\n",
    "    set_random_seed(seed)\n",
    "    print(f\"Training with seed: {seed}\")\n",
    "    \n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = SetGNN(args, tokenizer).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args[\"lr\"])\n",
    "    \n",
    "    best_test_metric, best_test_long_seq_metric = train_with_early_stopping(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        test_dataloader,\n",
    "        optimizer, \n",
    "        loss_fn, \n",
    "        device, \n",
    "        args,\n",
    "        val_long_seq_idx,\n",
    "        test_long_seq_idx,\n",
    "        task_type=task_type,\n",
    "        eval_metric = \"prauc\")\n",
    "    \n",
    "    final_metrics.append(best_test_metric)\n",
    "    final_long_seq_metrics.append(best_test_long_seq_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9abd8e-1f4b-47bf-bed3-337489bcc79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def topk_avg_performance_formatted(performances, long_seq_performances, k=5):\n",
    "    metrics = [\"f1\", \"auc\", \"prauc\"]\n",
    "    scores = {m: np.array([p[m] for p in performances]) for m in metrics}\n",
    "\n",
    "    # 计算排名（值越大排名越靠前）\n",
    "    ranks = {m: (-scores[m]).argsort().argsort() + 1 for m in metrics}\n",
    "    avg_ranks = np.mean(np.stack([ranks[m] for m in metrics], axis=1), axis=1)\n",
    "\n",
    "    # 选 top-k\n",
    "    topk_idx = np.argsort(avg_ranks)[:k]\n",
    "    final_avg = {m: np.mean([performances[i][m] for i in topk_idx]) for m in performances[0].keys()}\n",
    "    final_std = {m: np.std([performances[i][m] for i in topk_idx], ddof=0) for m in performances[0].keys()}\n",
    "    final_long_seq_avg = {m: np.mean([long_seq_performances[i][m] for i in topk_idx]) for m in long_seq_performances[0].keys()}\n",
    "    final_long_seq_std = {m: np.std([long_seq_performances[i][m] for i in topk_idx], ddof=0) for m in long_seq_performances[0].keys()}\n",
    "\n",
    "    # 打印结果（转百分比，均保留两位小数）\n",
    "    print(\"Final Metrics:\")\n",
    "    for m in performances[0].keys():\n",
    "        mean_val = final_avg[m] * 100\n",
    "        std_val = final_std[m] * 100\n",
    "        print(f\"{m}: {mean_val:.2f} ± {std_val:.2f}\")\n",
    "    print(\"\\nFinal Long Sequence Metrics:\")\n",
    "    for m in long_seq_performances[0].keys():\n",
    "        mean_val = final_long_seq_avg[m] * 100\n",
    "        std_val = final_long_seq_std[m] * 100\n",
    "        print(f\"{m}: {mean_val:.2f} ± {std_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4b92e-35fc-46b9-8f3d-4713b4c71cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_per_class_performance(dfs, col_name=\"prauc\"):\n",
    "    \"\"\"\n",
    "    输入一个 DataFrame 列表，对每个疾病在所有表格的指定列计算 mean ± std 并打印。\n",
    "\n",
    "    参数:\n",
    "        dfs (list[pd.DataFrame]): 多个表格组成的列表\n",
    "        col_name (str): 要计算的指标列名 (默认: \"prauc\")\n",
    "    \"\"\"\n",
    "    # 拼接所有表格\n",
    "    all_values = pd.concat(dfs, axis=0)\n",
    "\n",
    "    # 按疾病分组，计算 mean 和 std\n",
    "    grouped = all_values.groupby(all_values.index)[col_name].agg([\"mean\", \"std\"])\n",
    "\n",
    "    # 打印\n",
    "    for disease, row in grouped.iterrows():\n",
    "        mean_val = row[\"mean\"] * 100\n",
    "        std_val = row[\"std\"] * 100\n",
    "        print(f\"{disease}: {mean_val:.2f} ± {std_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55c99e-6f60-468f-a7a7-1b67f170d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_type == \"binary\":\n",
    "    topk_avg_performance_formatted(final_metrics, final_long_seq_metrics)\n",
    "else:\n",
    "    final_metrics_global = [metrics[\"global\"] for metrics in final_metrics]\n",
    "    final_metrics_per_class = [metrics[\"per_class\"] for metrics in final_metrics]\n",
    "    final_long_seq_metrics_global = [metrics[\"global\"] for metrics in final_long_seq_metrics]\n",
    "    final_long_seq_metrics_per_class = [metrics[\"per_class\"] for metrics in final_long_seq_metrics]\n",
    "    topk_avg_performance_formatted(final_metrics_global, final_long_seq_metrics_global)\n",
    "    print(\"\\nPer-class performance, all patients:\")\n",
    "    print_per_class_performance(final_metrics_per_class, col_name=\"prauc\")\n",
    "    print(\"\\nPer-class performance, long seq:\")\n",
    "    print_per_class_performance(final_long_seq_metrics_per_class, col_name=\"prauc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heart-cuda",
   "language": "python",
   "name": "heart-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
