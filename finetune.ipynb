{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc53801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 1.13.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from heterogt.utils.tokenizer import EHRTokenizer\n",
    "from heterogt.utils.dataset import FineTuneEHRDataset, batcher, expand_level3\n",
    "from heterogt.utils.train import train_with_early_stopping\n",
    "from heterogt.utils.seed import set_random_seed\n",
    "from heterogt.model.model import HeteroGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42d3e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random seed set to 123\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af3d65aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    dataset = \"MIMIC-III\",\n",
    "    tasks = [\"death\", \"readmission\", \"stay\", \"next_diag_6m\", \"next_diag_12m\"], \n",
    "    task_index = 2,  # index of the task to train\n",
    "    token_type = [\"diag\", \"med\", \"lab\", \"pro\"],\n",
    "    special_tokens = [\"[PAD]\", \"[CLS]\"],\n",
    "    attn_mask_dicts = [{1:[6,7], 2:[6,7], 3:[6,7], 4:[6,7], 6:[2,3,4,5,6,7], 7:[2,3,4,5,6,7]}, \n",
    "                       {1:[6,7], 2:[6,7], 3:[6,7], 4:[6,7], 6:[2,3,4,5,6,7], 7:[2,3,4,5,6,7]}],\n",
    "    d_model = 64,\n",
    "    num_heads = 4,\n",
    "    batch_size = 32,\n",
    "    lr = 1e-3,\n",
    "    epochs = 500,\n",
    "    early_stop_patience = 5,\n",
    "    group_code_thre = 5,  # if there are group_code_thre diag codes belongs to the same group ICD code, then the group code is generated\n",
    "    use_pretrained_model = True,\n",
    "    pretrain_mask_rate = 0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe56943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current task: stay\n"
     ]
    }
   ],
   "source": [
    "full_data_path = f\"./data_process/{config.dataset}-processed/mimic.pkl\"  # for tokenizer\n",
    "curr_task = config.tasks[config.task_index]\n",
    "print(\"Current task:\", curr_task)\n",
    "if curr_task == \"next_diag_6m\":\n",
    "    finetune_data_path = f\"./data_process/{config.dataset}-processed/mimic_nextdiag_6m.pkl\"\n",
    "elif curr_task == \"next_diag_12m\":\n",
    "    finetune_data_path = f\"./data_process/{config.dataset}-processed/mimic_nextdiag_12m.pkl\"\n",
    "else:\n",
    "    finetune_data_path = f\"./data_process/{config.dataset}-processed/mimic_downstream.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d4378a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max admissions per patient: 8\n"
     ]
    }
   ],
   "source": [
    "ehr_full_data = pickle.load(open(full_data_path, 'rb'))\n",
    "group_code_sentences = [expand_level3()[1]]\n",
    "diag_sentences = ehr_full_data[\"ICD9_CODE\"].values.tolist()\n",
    "med_sentences = ehr_full_data[\"NDC\"].values.tolist()\n",
    "lab_sentences = ehr_full_data[\"LAB_TEST\"].values.tolist()\n",
    "pro_sentences = ehr_full_data[\"PRO_CODE\"].values.tolist()\n",
    "age_sentences = [[str(c)] for c in set(ehr_full_data[\"AGE\"].values.tolist())] # important of [[]]\n",
    "max_admissions = ehr_full_data.groupby(\"SUBJECT_ID\")[\"HADM_ID\"].nunique().max()\n",
    "config.max_num_adms = max_admissions\n",
    "print(f\"Max admissions per patient: {config.max_num_adms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8db4baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age vocabulary size: 18\n",
      "Group code vocabulary size: 19\n"
     ]
    }
   ],
   "source": [
    "tokenizer = EHRTokenizer(age_sentences, group_code_sentences, diag_sentences, med_sentences, lab_sentences, \n",
    "                         pro_sentences, special_tokens=config.special_tokens)\n",
    "config.label_vocab_size = len(tokenizer.diag_voc.id2word)  # only for diagnosis\n",
    "config.global_vocab_size = len(tokenizer.vocab.id2word)\n",
    "config.age_vocab_size = tokenizer.token_number(\"age\")\n",
    "config.group_code_vocab_size = tokenizer.token_number(\"group\")\n",
    "print(f\"Age vocabulary size: {config.age_vocab_size}\")\n",
    "print(f\"Group code vocabulary size: {config.group_code_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a50b960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of DEATH in test dataset: 28.648477157360407 %\n",
      "Percentage of READMISSION in test dataset: 40.1491116751269 %\n",
      "Percentage of STAY>7 days in test dataset: 50.58692893401015 %\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = pickle.load(open(finetune_data_path, 'rb'))\n",
    "# example label percentage\n",
    "print(\"Percentage of DEATH in test dataset:\",\n",
    "      (test_data[\"DEATH\"] == True).mean() * 100, \"%\")\n",
    "\n",
    "print(\"Percentage of READMISSION in test dataset:\",\n",
    "      (test_data[\"READMISSION\"] == 1).mean() * 100, \"%\")\n",
    "\n",
    "print(\"Percentage of STAY>7 days in test dataset:\",\n",
    "      (test_data[\"STAY_DAYS\"] > 7).mean() * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f0ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FineTuneEHRDataset(train_data, tokenizer, token_type=config.token_type, task=curr_task, \n",
    "                                   max_num_adms=config.max_num_adms, group_code_thre=config.group_code_thre)\n",
    "val_dataset = FineTuneEHRDataset(val_data, tokenizer, token_type=config.token_type, task=curr_task, \n",
    "                                 max_num_adms=config.max_num_adms, group_code_thre=config.group_code_thre)\n",
    "test_dataset = FineTuneEHRDataset(test_data, tokenizer, token_type=config.token_type, task=curr_task, \n",
    "                                   max_num_adms=config.max_num_adms, group_code_thre=config.group_code_thre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c78018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean group token numer per patient 0.7971893963589908\n"
     ]
    }
   ],
   "source": [
    "num_group_code = []\n",
    "for i in range(len(train_dataset)):\n",
    "    input_ids, token_types, adm_index, age_ids, diag_group_codes, labels = train_dataset[i]\n",
    "    count = (token_types[0] == 6).sum().item()\n",
    "    num_group_code.append(count)\n",
    "print(\"Mean group token numer per patient\", np.mean(num_group_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d71f325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=batcher(tokenizer, n_token_type=len(config.token_type), is_pretrain = False),\n",
    "    shuffle=True,\n",
    "    batch_size=config.batch_size,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=batcher(tokenizer, n_token_type=len(config.token_type), is_pretrain = False),\n",
    "    shuffle=False,\n",
    "    batch_size=config.batch_size,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=batcher(tokenizer, n_token_type=len(config.token_type), is_pretrain = False),\n",
    "    shuffle=False,\n",
    "    batch_size=config.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b46906f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pass!\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    pass  # just to check if the dataloader works\n",
    "for batch in val_dataloader:\n",
    "    pass  # just to check if the dataloader works\n",
    "for batch in test_dataloader:\n",
    "    pass  # just to check if the dataloader works\n",
    "print(\"All pass!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10339a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if curr_task in [\"death\", \"stay\", \"readmission\"]:\n",
    "    eval_metric = \"f1\"\n",
    "    task_type = \"binary\"\n",
    "    loss_fn = F.binary_cross_entropy_with_logits\n",
    "else:\n",
    "    eval_metric = \"f1\"\n",
    "    task_type = \"l2r\"\n",
    "    loss_fn = lambda x, y: F.binary_cross_entropy_with_logits(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5717b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 293])\n",
      "Token Types shape: torch.Size([32, 293])\n",
      "Admission Index shape: torch.Size([32, 293])\n",
      "Age IDs shape: torch.Size([32, 8])\n",
      "Diag Code Group Dict number: 32\n",
      "Labels shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "input_ids, token_types, adm_index, age_ids, diag_code_group_dicts, labels = next(iter(train_dataloader))\n",
    "print(\"Input IDs shape:\", input_ids.shape)\n",
    "print(\"Token Types shape:\", token_types.shape)\n",
    "print(\"Admission Index shape:\", adm_index.shape)\n",
    "print(\"Age IDs shape:\", age_ids.shape)\n",
    "print(\"Diag Code Group Dict number:\", len(diag_code_group_dicts))\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e9e16",
   "metadata": {},
   "source": [
    "# Model Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af42068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIMIC-III-0.7-64\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "if config.use_pretrained_model:\n",
    "    pretrain_exp_name = (\n",
    "    f\"{config.dataset}-{config.pretrain_mask_rate}-{config.d_model}\"\n",
    ")\n",
    "    print(pretrain_exp_name)\n",
    "    save_path = \"./pretrained_models/\" + pretrain_exp_name\n",
    "    state_dict = torch.load(f\"{save_path}/pretrained_model.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fed5f6-b245-4bc3-b252-f6af0756f18a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Missing keys: ['cls_head.cls.0.weight', 'cls_head.cls.0.bias', 'cls_head.cls.2.weight', 'cls_head.cls.2.bias']\n",
      "[Warning] Unexpected keys: ['diag_cls.cls.0.weight', 'diag_cls.cls.0.bias', 'diag_cls.cls.2.weight', 'diag_cls.cls.2.bias', 'med_cls.cls.0.weight', 'med_cls.cls.0.bias', 'med_cls.cls.2.weight', 'med_cls.cls.2.bias', 'lab_cls.cls.0.weight', 'lab_cls.cls.0.bias', 'lab_cls.cls.2.weight', 'lab_cls.cls.2.bias', 'pro_cls.cls.0.weight', 'pro_cls.cls.0.bias', 'pro_cls.cls.2.weight', 'pro_cls.cls.2.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001: 100%|██████████| 98/98 [00:06<00:00, 16.04it/s, loss=0.6085]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.11it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.6642874723094653, 'recall': 0.8463468171814164, 'f1': 0.744346382274101, 'auc': 0.7999988244595604, 'prauc': 0.8094198590348712}\n",
      "Test:      {'precision': 0.647897535039517, 'recall': 0.8407024145473795, 'f1': 0.7318138343066659, 'auc': 0.7946282037924304, 'prauc': 0.8075749247729611}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 002: 100%|██████████| 98/98 [00:06<00:00, 16.14it/s, loss=0.5531]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.15it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.6949920085223895, 'recall': 0.818124804011232, 'f1': 0.7515483170819328, 'auc': 0.8156240677939195, 'prauc': 0.8245096525466373}\n",
      "Test:      {'precision': 0.6898917920277385, 'recall': 0.8196926936317978, 'f1': 0.7492118035759486, 'auc': 0.8115886924706568, 'prauc': 0.8220086897453106}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 003: 100%|██████████| 98/98 [00:05<00:00, 17.03it/s, loss=0.4993]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.17it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.7821482602088421, 'recall': 0.648479147066013, 'f1': 0.7090690847034848, 'auc': 0.8208141290717279, 'prauc': 0.8342775500063317}\n",
      "Test:      {'precision': 0.7778189910950378, 'recall': 0.6575729068652946, 'f1': 0.7126592983461761, 'auc': 0.815756359516335, 'prauc': 0.830082850211411}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 004: 100%|██████████| 98/98 [00:06<00:00, 16.15it/s, loss=0.4878]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.10it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.813736903372861, 'recall': 0.6575729068652946, 'f1': 0.7273673207561985, 'auc': 0.8404218421843994, 'prauc': 0.8493777252503707}\n",
      "Test:      {'precision': 0.7993920972614005, 'recall': 0.6597679523340867, 'f1': 0.7228998404306879, 'auc': 0.8330054606852306, 'prauc': 0.8457134540843725}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 005: 100%|██████████| 98/98 [00:05<00:00, 16.96it/s, loss=0.4409]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.15it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 22.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.7762879322484958, 'recall': 0.6898714330489499, 'f1': 0.730532952013118, 'auc': 0.8306717389729534, 'prauc': 0.8364994618912593}\n",
      "Test:      {'precision': 0.7769936485505399, 'recall': 0.6904985888971763, 'f1': 0.7311970728831141, 'auc': 0.8303852478448438, 'prauc': 0.8385976648394879}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 006: 100%|██████████| 98/98 [00:06<00:00, 16.14it/s, loss=0.4076]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.09it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.7780033840921219, 'recall': 0.7209156475361527, 'f1': 0.7483723908381501, 'auc': 0.83363184022617, 'prauc': 0.8414342905047226}\n",
      "Test:      {'precision': 0.7695711368251547, 'recall': 0.7089996864198527, 'f1': 0.7380447150973809, 'auc': 0.8331497669305653, 'prauc': 0.8458033945092314}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 007: 100%|██████████| 98/98 [00:05<00:00, 16.95it/s, loss=0.3619]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.13it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.7720637027180755, 'recall': 0.7296958294113212, 'f1': 0.7502821165556474, 'auc': 0.8356290030542952, 'prauc': 0.8433847143084795}\n",
      "Test:      {'precision': 0.776776776774185, 'recall': 0.7300094073354343, 'f1': 0.7526673082904533, 'auc': 0.8385338445207167, 'prauc': 0.8508987826563991}\n",
      "\n",
      "Early stopping triggered (no improvement for 5 epochs).\n",
      "\n",
      "Best validation performance:\n",
      "{'precision': 0.6949920085223895, 'recall': 0.818124804011232, 'f1': 0.7515483170819328, 'auc': 0.8156240677939195, 'prauc': 0.8245096525466373}\n",
      "Corresponding test performance:\n",
      "{'precision': 0.6898917920277385, 'recall': 0.8196926936317978, 'f1': 0.7492118035759486, 'auc': 0.8115886924706568, 'prauc': 0.8220086897453106}\n",
      "[Warning] Missing keys: ['cls_head.cls.0.weight', 'cls_head.cls.0.bias', 'cls_head.cls.2.weight', 'cls_head.cls.2.bias']\n",
      "[Warning] Unexpected keys: ['diag_cls.cls.0.weight', 'diag_cls.cls.0.bias', 'diag_cls.cls.2.weight', 'diag_cls.cls.2.bias', 'med_cls.cls.0.weight', 'med_cls.cls.0.bias', 'med_cls.cls.2.weight', 'med_cls.cls.2.bias', 'lab_cls.cls.0.weight', 'lab_cls.cls.0.bias', 'lab_cls.cls.2.weight', 'lab_cls.cls.2.bias', 'pro_cls.cls.0.weight', 'pro_cls.cls.0.bias', 'pro_cls.cls.2.weight', 'pro_cls.cls.2.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001: 100%|██████████| 98/98 [00:06<00:00, 16.15it/s, loss=0.6224]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 22.55it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.6823218997343474, 'recall': 0.8109125117566294, 'f1': 0.7410803790441262, 'auc': 0.8000816146580448, 'prauc': 0.8112115005505669}\n",
      "Test:      {'precision': 0.6693569382955757, 'recall': 0.806208842894932, 'f1': 0.7314366948986802, 'auc': 0.7907554912628533, 'prauc': 0.8042334924191312}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 002: 100%|██████████| 98/98 [00:05<00:00, 16.87it/s, loss=0.5453]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.00it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.6655868427593681, 'recall': 0.8375666353062479, 'f1': 0.7417384010617254, 'auc': 0.7990688413561958, 'prauc': 0.8040598389655313}\n",
      "Test:      {'precision': 0.6631944444427996, 'recall': 0.8385073690785874, 'f1': 0.7406176380518293, 'auc': 0.7898045901164064, 'prauc': 0.7950201369372583}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 003: 100%|██████████| 98/98 [00:06<00:00, 16.20it/s, loss=0.5152]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.13it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.8216003497996432, 'recall': 0.5892129194086259, 'f1': 0.6862673435626656, 'auc': 0.8253521672997002, 'prauc': 0.837791060642445}\n",
      "Test:      {'precision': 0.821865766142584, 'recall': 0.6105362182483207, 'f1': 0.700611725944743, 'auc': 0.8249579840815162, 'prauc': 0.8394433784592801}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 004: 100%|██████████| 98/98 [00:05<00:00, 16.98it/s, loss=0.4696]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.16it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.6930614406761307, 'recall': 0.8206334274041373, 'f1': 0.7514716389673122, 'auc': 0.8189625021689726, 'prauc': 0.8307285138298998}\n",
      "Test:      {'precision': 0.6904512067138237, 'recall': 0.8253370962658346, 'f1': 0.7518925818107639, 'auc': 0.8196312363879246, 'prauc': 0.8312859553342554}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 005: 100%|██████████| 98/98 [00:05<00:00, 16.50it/s, loss=0.4475]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 22.37it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.7157636809128348, 'recall': 0.8243963624934951, 'f1': 0.7662489020468327, 'auc': 0.8326542829007395, 'prauc': 0.8401677119849806}\n",
      "Test:      {'precision': 0.7007240547044765, 'recall': 0.8193791157076846, 'f1': 0.7554206368322705, 'auc': 0.827707252106081, 'prauc': 0.8405723101906195}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 006: 100%|██████████| 98/98 [00:05<00:00, 16.96it/s, loss=0.3978]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.13it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 22.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.7004405286325461, 'recall': 0.847601128877869, 'f1': 0.7670261017399431, 'auc': 0.8314291587681903, 'prauc': 0.8373889015668229}\n",
      "Test:      {'precision': 0.6881409413602732, 'recall': 0.8206334274041373, 'f1': 0.7485697890867592, 'auc': 0.8238585486727802, 'prauc': 0.8360179499001839}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 007: 100%|██████████| 98/98 [00:05<00:00, 16.98it/s, loss=0.3808]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 22.36it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.7201959716910175, 'recall': 0.8297271872034189, 'f1': 0.7710913544865147, 'auc': 0.8382288061864374, 'prauc': 0.8438743479141599}\n",
      "Test:      {'precision': 0.7145981410587354, 'recall': 0.8196926936317978, 'f1': 0.7635460735957874, 'auc': 0.8352034758326048, 'prauc': 0.8464026818815882}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 008: 100%|██████████| 98/98 [00:05<00:00, 16.90it/s, loss=0.3411]\n",
      "Running inference: 100%|██████████| 198/198 [00:08<00:00, 23.11it/s]\n",
      "Running inference: 100%|██████████| 197/197 [00:08<00:00, 23.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'precision': 0.7438042131327639, 'recall': 0.7529005957956949, 'f1': 0.7483247573478603, 'auc': 0.8280450591683663, 'prauc': 0.839134001537688}\n",
      "Test:      {'precision': 0.7416487894552816, 'recall': 0.7588585763538449, 'f1': 0.7501549856988905, 'auc': 0.8284085492516158, 'prauc': 0.8414861399584697}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 009:  88%|████████▊ | 86/98 [00:05<00:00, 16.78it/s, loss=0.3142]"
     ]
    }
   ],
   "source": [
    "final_metrics = []\n",
    "for i in range(10):\n",
    "    model = HeteroGT(tokenizer=tokenizer, token_types=config.token_type, d_model=config.d_model, num_heads=config.num_heads, layer_types=['gnn', 'tf', 'gnn', 'tf'], max_num_adms=config.max_num_adms, \n",
    "                     device=device, task=curr_task, label_vocab_size=config.label_vocab_size, attn_mask_dicts=attn_mask_dicts,\n",
    "                     use_cls_cat=True).to(device)\n",
    "    if config.use_pretrained_model:\n",
    "        model.load_weight(state_dict)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "    best_test_metric = train_with_early_stopping(model, train_dataloader, val_dataloader, test_dataloader,\n",
    "                                             optimizer, loss_fn, device, config.early_stop_patience, task_type, config.epochs, \n",
    "                                             val_long_seq_idx=None, test_long_seq_idx=None, eval_metric=eval_metric, return_model=False)\n",
    "    final_metrics.append(best_test_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_avg_performance_formatted(performances, k=5):\n",
    "    metrics = [\"f1\", \"auc\", \"prauc\"]\n",
    "    scores = {m: np.array([p[m] for p in performances]) for m in metrics}\n",
    "\n",
    "    # 计算排名（值越大排名越靠前）\n",
    "    ranks = {m: (-scores[m]).argsort().argsort() + 1 for m in metrics}\n",
    "    avg_ranks = np.mean(np.stack([ranks[m] for m in metrics], axis=1), axis=1)\n",
    "\n",
    "    # 选 top-k\n",
    "    topk_idx = np.argsort(avg_ranks)[:k]\n",
    "    final_avg = {m: np.mean([performances[i][m] for i in topk_idx]) for m in performances[0].keys()}\n",
    "    final_std = {m: np.std([performances[i][m] for i in topk_idx], ddof=0) for m in performances[0].keys()}\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"Final Metrics:\")\n",
    "    for m in performances[0].keys():\n",
    "        print(f\"{m}: {final_avg[m]:.4f}±{final_std[m]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12dd11d-4936-466a-9a97-e10f223539fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topk_avg_performance_formatted(final_metrics, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
