{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237db178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 1.13.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from argparse import Namespace\n",
    "from torch.utils.data import DataLoader\n",
    "from heterogt.utils.tokenizer import EHRTokenizer\n",
    "from heterogt.utils.dataset import PreTrainEHRDataset, batcher, expand_level3\n",
    "from heterogt.utils.seed import set_random_seed\n",
    "from heterogt.model.model import HeteroGTPreTrain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3c4dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random seed set to 123\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af749748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71b0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    dataset = \"MIMIC-IV\",\n",
    "    tasks = [\"death\", \"readmission\", \"stay\", \"next_diag_6m\", \"next_diag_12m\"], \n",
    "    task_index = 2,  # index of the task to train\n",
    "    token_type = [\"diag\", \"med\", \"lab\", \"pro\"],\n",
    "    special_tokens = [\"[PAD]\", \"[CLS]\"],\n",
    "    attn_mask_dicts = [{1:[6,7], 2:[6,7], 3:[6,7], 4:[6,7], 5:[6,7], 6:[2,3,4,5,6,7], 7:[2,3,4,5,6,7]}, \n",
    "                       {1:[6,7], 2:[6,7], 3:[6,7], 4:[6,7], 5:[6,7], 6:[2,3,4,5,6,7], 7:[2,3,4,5,6,7]}],\n",
    "    d_model = 64,\n",
    "    num_heads = 4,\n",
    "    batch_size = 32,\n",
    "    lr = 1e-3,\n",
    "    epochs = 25,\n",
    "    early_stop_patience = 5,\n",
    "    group_code_thre = 5,  # if there are group_code_thre diag codes belongs to the same group ICD code, then the group code is generated\n",
    "    pretrain_mask_rate = 0.7,\n",
    "    cls_ontology_weight = 5e-2,\n",
    "    visit_ontology_weight = 5e-2,\n",
    "    adm_type_weight = 5e-2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92817ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path = f\"./data_process/{config.dataset}-processed/mimic.pkl\"  # for tokenizer\n",
    "pretrain_data_path = f\"./data_process/{config.dataset}-processed/mimic_pretrain.pkl\" # for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95fa33dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max admissions per patient: 8\n"
     ]
    }
   ],
   "source": [
    "ehr_full_data = pickle.load(open(full_data_path, 'rb'))\n",
    "group_code_sentences = [expand_level3()[1]]\n",
    "diag_sentences = ehr_full_data[\"ICD9_CODE\"].values.tolist()\n",
    "med_sentences = ehr_full_data[\"NDC\"].values.tolist()\n",
    "lab_sentences = ehr_full_data[\"LAB_TEST\"].values.tolist()\n",
    "pro_sentences = ehr_full_data[\"PRO_CODE\"].values.tolist()\n",
    "age_sentences = [[str(c)] for c in set(ehr_full_data[\"AGE\"].values.tolist())] # important of [[]]\n",
    "adm_type_sentences = ehr_full_data[\"ADMISSION_TYPE\"].values.tolist()\n",
    "max_admissions = ehr_full_data.groupby(\"SUBJECT_ID\")[\"HADM_ID\"].nunique().max()\n",
    "config.max_num_adms = max_admissions\n",
    "print(f\"Max admissions per patient: {config.max_num_adms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f7a74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age vocabulary size: 20\n",
      "Group code vocabulary size: 19\n"
     ]
    }
   ],
   "source": [
    "tokenizer = EHRTokenizer(age_sentences, group_code_sentences, diag_sentences, med_sentences, lab_sentences, \n",
    "                         pro_sentences, special_tokens=config.special_tokens, adm_types_sentences=adm_type_sentences)\n",
    "config.label_vocab_size = {\"diag\":tokenizer.token_number(\"diag\"),     \n",
    "                            \"med\":tokenizer.token_number(\"med\"), \n",
    "                            \"lab\":tokenizer.token_number(\"lab\"), \n",
    "                            \"pro\":tokenizer.token_number(\"pro\")}\n",
    "config.global_vocab_size = len(tokenizer.vocab.id2word)\n",
    "config.age_vocab_size = tokenizer.token_number(\"age\")\n",
    "config.group_code_vocab_size = tokenizer.token_number(\"group\")\n",
    "print(f\"Age vocabulary size: {config.age_vocab_size}\")\n",
    "print(f\"Group code vocabulary size: {config.group_code_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8248322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrain data\n",
    "ehr_pretrain_data = pickle.load(open(pretrain_data_path, 'rb'))\n",
    "# load occurence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e5676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pretrain samples: 42496\n"
     ]
    }
   ],
   "source": [
    "pretrain_dataset = PreTrainEHRDataset(ehr_pretrain_data=ehr_pretrain_data, tokenizer=tokenizer, token_type=config.token_type,\n",
    "                                      mask_rate=config.pretrain_mask_rate, group_code_thre=config.group_code_thre, max_num_adms=config.max_num_adms)\n",
    "print(\"Number of pretrain samples:\", len(pretrain_dataset))\n",
    "pretrain_dataloader = DataLoader(pretrain_dataset, batch_size=config.batch_size, \n",
    "                                 collate_fn=batcher(tokenizer, n_token_type=len(config.token_type), is_pretrain=True), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac9224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGTPreTrain(tokenizer=tokenizer, token_types=config.token_type, d_model=config.d_model, num_heads=config.num_heads, layer_types=['gnn', 'tf', 'gnn', 'tf'], max_num_adms=config.max_num_adms, \n",
    "                     device=device, label_vocab_size=config.label_vocab_size, attn_mask_dicts=config.attn_mask_dicts,\n",
    "                     use_cls_cat=True).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb79c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diag': 1.0, 'med': 1.0, 'lab': 1.0, 'pro': 1.0, 'cls_ontology': 0.05, 'visit_ontology': 0.05, 'adm_type': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# 统一权重表：未指定的一律 1.0\n",
    "loss_weights = {\n",
    "    **{t: 1.0 for t in config.token_type},\n",
    "    \"cls_ontology\": float(config.cls_ontology_weight),\n",
    "    \"visit_ontology\": float(config.visit_ontology_weight),\n",
    "    \"adm_type\": float(config.adm_type_weight)\n",
    "}\n",
    "loss_types = list(loss_weights.keys())\n",
    "print(loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "809a9fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001: 100%|██████████| 1328/1328 [01:16<00:00, 17.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Weighted total loss: 0.2012\n",
      "  Raw losses      | diag: 0.0227, med: 0.0594, lab: 0.0453, pro: 0.0185, cls_ontology: 0.4284, visit_ontology: 0.4298, adm_type: 0.2459\n",
      "  Contributions   | diag: 0.0227, med: 0.0594, lab: 0.0453, pro: 0.0185, cls_ontology: 0.0214, visit_ontology: 0.0215, adm_type: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 002: 100%|██████████| 1328/1328 [01:18<00:00, 16.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 002] Weighted total loss: 0.1672\n",
      "  Raw losses      | diag: 0.0175, med: 0.0499, lab: 0.0379, pro: 0.0120, cls_ontology: 0.3995, visit_ontology: 0.4003, adm_type: 0.1978\n",
      "  Contributions   | diag: 0.0175, med: 0.0499, lab: 0.0379, pro: 0.0120, cls_ontology: 0.0200, visit_ontology: 0.0200, adm_type: 0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 003: 100%|██████████| 1328/1328 [01:16<00:00, 17.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 003] Weighted total loss: 0.1605\n",
      "  Raw losses      | diag: 0.0170, med: 0.0480, lab: 0.0366, pro: 0.0108, cls_ontology: 0.3873, visit_ontology: 0.3902, adm_type: 0.1824\n",
      "  Contributions   | diag: 0.0170, med: 0.0480, lab: 0.0366, pro: 0.0108, cls_ontology: 0.0194, visit_ontology: 0.0195, adm_type: 0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 004: 100%|██████████| 1328/1328 [01:18<00:00, 16.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 004] Weighted total loss: 0.1564\n",
      "  Raw losses      | diag: 0.0166, med: 0.0467, lab: 0.0360, pro: 0.0102, cls_ontology: 0.3802, visit_ontology: 0.3817, adm_type: 0.1766\n",
      "  Contributions   | diag: 0.0166, med: 0.0467, lab: 0.0360, pro: 0.0102, cls_ontology: 0.0190, visit_ontology: 0.0191, adm_type: 0.0088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 005: 100%|██████████| 1328/1328 [01:16<00:00, 17.31batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 005] Weighted total loss: 0.1538\n",
      "  Raw losses      | diag: 0.0164, med: 0.0459, lab: 0.0356, pro: 0.0097, cls_ontology: 0.3767, visit_ontology: 0.3772, adm_type: 0.1725\n",
      "  Contributions   | diag: 0.0164, med: 0.0459, lab: 0.0356, pro: 0.0097, cls_ontology: 0.0188, visit_ontology: 0.0189, adm_type: 0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 006: 100%|██████████| 1328/1328 [01:17<00:00, 17.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 006] Weighted total loss: 0.1518\n",
      "  Raw losses      | diag: 0.0162, med: 0.0453, lab: 0.0352, pro: 0.0093, cls_ontology: 0.3730, visit_ontology: 0.3734, adm_type: 0.1698\n",
      "  Contributions   | diag: 0.0162, med: 0.0453, lab: 0.0352, pro: 0.0093, cls_ontology: 0.0187, visit_ontology: 0.0187, adm_type: 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 007: 100%|██████████| 1328/1328 [01:18<00:00, 16.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 007] Weighted total loss: 0.1501\n",
      "  Raw losses      | diag: 0.0160, med: 0.0448, lab: 0.0349, pro: 0.0090, cls_ontology: 0.3700, visit_ontology: 0.3698, adm_type: 0.1675\n",
      "  Contributions   | diag: 0.0160, med: 0.0448, lab: 0.0349, pro: 0.0090, cls_ontology: 0.0185, visit_ontology: 0.0185, adm_type: 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 008: 100%|██████████| 1328/1328 [01:16<00:00, 17.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 008] Weighted total loss: 0.1487\n",
      "  Raw losses      | diag: 0.0159, med: 0.0443, lab: 0.0347, pro: 0.0088, cls_ontology: 0.3676, visit_ontology: 0.3673, adm_type: 0.1655\n",
      "  Contributions   | diag: 0.0159, med: 0.0443, lab: 0.0347, pro: 0.0088, cls_ontology: 0.0184, visit_ontology: 0.0184, adm_type: 0.0083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 009: 100%|██████████| 1328/1328 [01:19<00:00, 16.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 009] Weighted total loss: 0.1476\n",
      "  Raw losses      | diag: 0.0158, med: 0.0440, lab: 0.0345, pro: 0.0086, cls_ontology: 0.3664, visit_ontology: 0.3654, adm_type: 0.1646\n",
      "  Contributions   | diag: 0.0158, med: 0.0440, lab: 0.0345, pro: 0.0086, cls_ontology: 0.0183, visit_ontology: 0.0183, adm_type: 0.0082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 010: 100%|██████████| 1328/1328 [01:14<00:00, 17.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 010] Weighted total loss: 0.1465\n",
      "  Raw losses      | diag: 0.0157, med: 0.0436, lab: 0.0343, pro: 0.0085, cls_ontology: 0.3639, visit_ontology: 0.3628, adm_type: 0.1627\n",
      "  Contributions   | diag: 0.0157, med: 0.0436, lab: 0.0343, pro: 0.0085, cls_ontology: 0.0182, visit_ontology: 0.0181, adm_type: 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 011: 100%|██████████| 1328/1328 [01:17<00:00, 17.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 011] Weighted total loss: 0.1458\n",
      "  Raw losses      | diag: 0.0156, med: 0.0435, lab: 0.0341, pro: 0.0084, cls_ontology: 0.3626, visit_ontology: 0.3612, adm_type: 0.1622\n",
      "  Contributions   | diag: 0.0156, med: 0.0435, lab: 0.0341, pro: 0.0084, cls_ontology: 0.0181, visit_ontology: 0.0181, adm_type: 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 012: 100%|██████████| 1328/1328 [01:17<00:00, 17.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 012] Weighted total loss: 0.1451\n",
      "  Raw losses      | diag: 0.0155, med: 0.0433, lab: 0.0340, pro: 0.0082, cls_ontology: 0.3619, visit_ontology: 0.3603, adm_type: 0.1608\n",
      "  Contributions   | diag: 0.0155, med: 0.0433, lab: 0.0340, pro: 0.0082, cls_ontology: 0.0181, visit_ontology: 0.0180, adm_type: 0.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 013: 100%|██████████| 1328/1328 [01:18<00:00, 16.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 013] Weighted total loss: 0.1444\n",
      "  Raw losses      | diag: 0.0154, med: 0.0430, lab: 0.0338, pro: 0.0082, cls_ontology: 0.3606, visit_ontology: 0.3591, adm_type: 0.1597\n",
      "  Contributions   | diag: 0.0154, med: 0.0430, lab: 0.0338, pro: 0.0082, cls_ontology: 0.0180, visit_ontology: 0.0180, adm_type: 0.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 014: 100%|██████████| 1328/1328 [01:17<00:00, 17.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 014] Weighted total loss: 0.1439\n",
      "  Raw losses      | diag: 0.0154, med: 0.0429, lab: 0.0337, pro: 0.0081, cls_ontology: 0.3597, visit_ontology: 0.3578, adm_type: 0.1590\n",
      "  Contributions   | diag: 0.0154, med: 0.0429, lab: 0.0337, pro: 0.0081, cls_ontology: 0.0180, visit_ontology: 0.0179, adm_type: 0.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 015: 100%|██████████| 1328/1328 [01:15<00:00, 17.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 015] Weighted total loss: 0.1432\n",
      "  Raw losses      | diag: 0.0153, med: 0.0427, lab: 0.0335, pro: 0.0080, cls_ontology: 0.3587, visit_ontology: 0.3566, adm_type: 0.1580\n",
      "  Contributions   | diag: 0.0153, med: 0.0427, lab: 0.0335, pro: 0.0080, cls_ontology: 0.0179, visit_ontology: 0.0178, adm_type: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 016: 100%|██████████| 1328/1328 [01:18<00:00, 16.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 016] Weighted total loss: 0.1428\n",
      "  Raw losses      | diag: 0.0153, med: 0.0426, lab: 0.0334, pro: 0.0080, cls_ontology: 0.3576, visit_ontology: 0.3556, adm_type: 0.1573\n",
      "  Contributions   | diag: 0.0153, med: 0.0426, lab: 0.0334, pro: 0.0080, cls_ontology: 0.0179, visit_ontology: 0.0178, adm_type: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 017: 100%|██████████| 1328/1328 [01:16<00:00, 17.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 017] Weighted total loss: 0.1423\n",
      "  Raw losses      | diag: 0.0152, med: 0.0424, lab: 0.0333, pro: 0.0079, cls_ontology: 0.3570, visit_ontology: 0.3552, adm_type: 0.1564\n",
      "  Contributions   | diag: 0.0152, med: 0.0424, lab: 0.0333, pro: 0.0079, cls_ontology: 0.0179, visit_ontology: 0.0178, adm_type: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 018: 100%|██████████| 1328/1328 [01:18<00:00, 16.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 018] Weighted total loss: 0.1419\n",
      "  Raw losses      | diag: 0.0152, med: 0.0422, lab: 0.0332, pro: 0.0079, cls_ontology: 0.3565, visit_ontology: 0.3549, adm_type: 0.1554\n",
      "  Contributions   | diag: 0.0152, med: 0.0422, lab: 0.0332, pro: 0.0079, cls_ontology: 0.0178, visit_ontology: 0.0177, adm_type: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 019: 100%|██████████| 1328/1328 [01:14<00:00, 17.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 019] Weighted total loss: 0.1416\n",
      "  Raw losses      | diag: 0.0151, med: 0.0422, lab: 0.0332, pro: 0.0079, cls_ontology: 0.3559, visit_ontology: 0.3541, adm_type: 0.1551\n",
      "  Contributions   | diag: 0.0151, med: 0.0422, lab: 0.0332, pro: 0.0079, cls_ontology: 0.0178, visit_ontology: 0.0177, adm_type: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 020: 100%|██████████| 1328/1328 [01:18<00:00, 16.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 020] Weighted total loss: 0.1412\n",
      "  Raw losses      | diag: 0.0151, med: 0.0420, lab: 0.0331, pro: 0.0078, cls_ontology: 0.3554, visit_ontology: 0.3533, adm_type: 0.1545\n",
      "  Contributions   | diag: 0.0151, med: 0.0420, lab: 0.0331, pro: 0.0078, cls_ontology: 0.0178, visit_ontology: 0.0177, adm_type: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 021: 100%|██████████| 1328/1328 [01:17<00:00, 17.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 021] Weighted total loss: 0.1409\n",
      "  Raw losses      | diag: 0.0151, med: 0.0419, lab: 0.0330, pro: 0.0078, cls_ontology: 0.3546, visit_ontology: 0.3529, adm_type: 0.1544\n",
      "  Contributions   | diag: 0.0151, med: 0.0419, lab: 0.0330, pro: 0.0078, cls_ontology: 0.0177, visit_ontology: 0.0176, adm_type: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 022: 100%|██████████| 1328/1328 [01:18<00:00, 16.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 022] Weighted total loss: 0.1405\n",
      "  Raw losses      | diag: 0.0151, med: 0.0418, lab: 0.0329, pro: 0.0078, cls_ontology: 0.3539, visit_ontology: 0.3517, adm_type: 0.1536\n",
      "  Contributions   | diag: 0.0151, med: 0.0418, lab: 0.0329, pro: 0.0078, cls_ontology: 0.0177, visit_ontology: 0.0176, adm_type: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 023: 100%|██████████| 1328/1328 [01:16<00:00, 17.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 023] Weighted total loss: 0.1402\n",
      "  Raw losses      | diag: 0.0150, med: 0.0416, lab: 0.0329, pro: 0.0077, cls_ontology: 0.3536, visit_ontology: 0.3514, adm_type: 0.1537\n",
      "  Contributions   | diag: 0.0150, med: 0.0416, lab: 0.0329, pro: 0.0077, cls_ontology: 0.0177, visit_ontology: 0.0176, adm_type: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 024: 100%|██████████| 1328/1328 [01:17<00:00, 17.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 024] Weighted total loss: 0.1399\n",
      "  Raw losses      | diag: 0.0150, med: 0.0415, lab: 0.0328, pro: 0.0077, cls_ontology: 0.3533, visit_ontology: 0.3511, adm_type: 0.1526\n",
      "  Contributions   | diag: 0.0150, med: 0.0415, lab: 0.0328, pro: 0.0077, cls_ontology: 0.0177, visit_ontology: 0.0176, adm_type: 0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 025: 100%|██████████| 1328/1328 [01:18<00:00, 17.02batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 025] Weighted total loss: 0.1398\n",
      "  Raw losses      | diag: 0.0150, med: 0.0416, lab: 0.0328, pro: 0.0077, cls_ontology: 0.3527, visit_ontology: 0.3504, adm_type: 0.1523\n",
      "  Contributions   | diag: 0.0150, med: 0.0416, lab: 0.0328, pro: 0.0077, cls_ontology: 0.0176, visit_ontology: 0.0175, adm_type: 0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1 + config.epochs):\n",
    "    model.train()\n",
    "    avg_total_loss = 0.0\n",
    "    avg_loss_dict = {t: 0.0 for t in loss_types}         # 未加权分项\n",
    "    avg_contrib_dict = {t: 0.0 for t in loss_types}      # 加权后贡献\n",
    "    step_iter = tqdm(pretrain_dataloader, desc=f\"Epoch {epoch:03d}\", unit=\"batch\")\n",
    "\n",
    "    for step, batch in enumerate(step_iter):\n",
    "        batch = [x.to(device) if isinstance(x, torch.Tensor) else x for x in batch]\n",
    "        loss_dict = model(*batch)   # {loss_type: tensor}\n",
    "\n",
    "        # 总 loss（加权）\n",
    "        total_loss = sum(loss_weights[t] * loss_dict[t] for t in loss_types)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累积\n",
    "        avg_total_loss += total_loss.item()\n",
    "        for t in loss_types:\n",
    "            avg_loss_dict[t] += loss_dict[t].item()                       # 原始 loss\n",
    "            avg_contrib_dict[t] += (loss_weights[t] * loss_dict[t]).item()  # 加权后贡献\n",
    "\n",
    "    steps = step + 1\n",
    "    avg_total_loss /= steps\n",
    "    avg_loss_dict = {t: v / steps for t, v in avg_loss_dict.items()}\n",
    "    avg_contrib_dict = {t: v / steps for t, v in avg_contrib_dict.items()}\n",
    "\n",
    "    # 日志字符串\n",
    "    raw_loss_str = \", \".join(f\"{t}: {avg_loss_dict[t]:.4f}\" for t in loss_types)\n",
    "    contrib_str  = \", \".join(f\"{t}: {avg_contrib_dict[t]:.4f}\" for t in loss_types)\n",
    "\n",
    "    print(f\"[Epoch {epoch:03d}] Weighted total loss: {avg_total_loss:.4f}\\n\"\n",
    "          f\"  Raw losses      | {raw_loss_str}\\n\"\n",
    "          f\"  Contributions   | {contrib_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a15c1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model: MIMIC-IV-0.7-64-0.05-0.05-0.05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "exp_name = (\n",
    "    f\"{config.dataset}-{config.pretrain_mask_rate}-{config.d_model}-{config.cls_ontology_weight}-{config.visit_ontology_weight}-{config.adm_type_weight}\"\n",
    ")\n",
    "save_path = \"./pretrained_models/\" + exp_name\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "torch.save(model.cpu().state_dict(), f\"{save_path}/pretrained_model.pt\")\n",
    "print(\"Save model:\", exp_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
