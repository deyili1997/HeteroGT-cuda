{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6eb0b1e-99d8-4f26-8941-8dc0d026cd58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_performance(A_lit, A_loc, B_lit, prevalence):\n",
    "    mean_B_loc_f1, std_B_loc_f1 = estimate_B_local(\n",
    "    mean_A_lit=A_lit[\"f1\"][0], std_A_lit=A_lit[\"f1\"][1],\n",
    "    mean_B_lit=B_lit[\"f1\"][0], std_B_lit=B_lit[\"f1\"][1],\n",
    "    mean_A_loc=A_loc[\"f1\"][0], std_A_loc=A_loc[\"f1\"][1])\n",
    "    \n",
    "    mean_B_loc_auroc, std_B_loc_auroc = estimate_B_local(\n",
    "    mean_A_lit=A_lit[\"auroc\"][0], std_A_lit=A_lit[\"auroc\"][1],\n",
    "    mean_B_lit=B_lit[\"auroc\"][0], std_B_lit=B_lit[\"auroc\"][1],\n",
    "    mean_A_loc=A_loc[\"auroc\"][0], std_A_loc=A_loc[\"auroc\"][1])\n",
    "        \n",
    "    auprc_mean2, auprc_std2 = estimate_auprc_from_f1_auroc(\n",
    "    f1_mean=mean_B_loc_f1, f1_std=std_B_loc_f1,\n",
    "    auc_mean=mean_B_loc_auroc, auc_std=std_B_loc_auroc,\n",
    "    prevalence=prevalence,          # 传入你的测试集正例比例\n",
    "    n_mc=4000,\n",
    "    seed=123\n",
    ")\n",
    "    \n",
    "    print(f\"推算本地 B f1: {mean_B_loc_f1 * 100:.2f} ± {std_B_loc_f1 * 100:.2f}\")\n",
    "    print(f\"推算本地 B AUROC: {mean_B_loc_auroc * 100:.2f} ± {std_B_loc_auroc * 100:.2f}\")\n",
    "    print(f\"推算本地 B AUPRC: {auprc_mean2 * 100:.2f} ± {auprc_std2 * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a42067-da59-4602-9806-d8b72fab222e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prevalence = 0.4015\n",
    "A_lit = {\"f1\": (0.6945, 0.002), \"auroc\": (0.7837, 0.003)}\n",
    "A_loc = {\"f1\": (0.6925, 0.0102), \"auroc\": (0.7807, 0.0049)}\n",
    "B_lit = {\"f1\": (0.6096, 0.008), \"auroc\": (0.6762, 0.005)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5834ba44-9017-41d9-be66-741ad42e6ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'estimate_B_local' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_lit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_lit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevalence\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mget_performance\u001b[0;34m(A_lit, A_loc, B_lit, prevalence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_performance\u001b[39m(A_lit, A_loc, B_lit, prevalence):\n\u001b[0;32m----> 2\u001b[0m     mean_B_loc_f1, std_B_loc_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_B_local\u001b[49m(\n\u001b[1;32m      3\u001b[0m     mean_A_lit\u001b[38;5;241m=\u001b[39mA_lit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], std_A_lit\u001b[38;5;241m=\u001b[39mA_lit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      4\u001b[0m     mean_B_lit\u001b[38;5;241m=\u001b[39mB_lit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], std_B_lit\u001b[38;5;241m=\u001b[39mB_lit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      5\u001b[0m     mean_A_loc\u001b[38;5;241m=\u001b[39mA_loc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], std_A_loc\u001b[38;5;241m=\u001b[39mA_loc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      7\u001b[0m     mean_B_loc_auroc, std_B_loc_auroc \u001b[38;5;241m=\u001b[39m estimate_B_local(\n\u001b[1;32m      8\u001b[0m     mean_A_lit\u001b[38;5;241m=\u001b[39mA_lit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], std_A_lit\u001b[38;5;241m=\u001b[39mA_lit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      9\u001b[0m     mean_B_lit\u001b[38;5;241m=\u001b[39mB_lit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], std_B_lit\u001b[38;5;241m=\u001b[39mB_lit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     10\u001b[0m     mean_A_loc\u001b[38;5;241m=\u001b[39mA_loc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], std_A_loc\u001b[38;5;241m=\u001b[39mA_loc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     12\u001b[0m     auprc_mean2, auprc_std2 \u001b[38;5;241m=\u001b[39m estimate_auprc_from_f1_auroc(\n\u001b[1;32m     13\u001b[0m     f1_mean\u001b[38;5;241m=\u001b[39mmean_B_loc_f1, f1_std\u001b[38;5;241m=\u001b[39mstd_B_loc_f1,\n\u001b[1;32m     14\u001b[0m     auc_mean\u001b[38;5;241m=\u001b[39mmean_B_loc_auroc, auc_std\u001b[38;5;241m=\u001b[39mstd_B_loc_auroc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'estimate_B_local' is not defined"
     ]
    }
   ],
   "source": [
    "get_performance(A_lit, A_loc, B_lit, prevalence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1622846-1c9b-4f6e-8ccd-c7dc056cfd3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def logit(x):\n",
    "    x = min(max(x, 1e-8), 1 - 1e-8)\n",
    "    return math.log(x / (1 - x))\n",
    "\n",
    "def inv_logit(u):\n",
    "    return 1 / (1 + math.exp(-u))\n",
    "\n",
    "def to_logit(mean, std):\n",
    "    u = logit(mean)\n",
    "    dm_du = mean * (1 - mean)\n",
    "    s_u = std / max(dm_du, 1e-8)\n",
    "    return u, s_u\n",
    "\n",
    "def from_logit(u, s_u):\n",
    "    m = inv_logit(u)\n",
    "    dm_du = m * (1 - m)\n",
    "    s_m = dm_du * s_u\n",
    "    return m, s_m\n",
    "\n",
    "def estimate_B_local(mean_A_lit, std_A_lit,\n",
    "                     mean_B_lit, std_B_lit,\n",
    "                     mean_A_loc, std_A_loc):\n",
    "    # 转到 logit 空间\n",
    "    uA_lit, sA_lit = to_logit(mean_A_lit, std_A_lit)\n",
    "    uB_lit, sB_lit = to_logit(mean_B_lit, std_B_lit)\n",
    "    uA_loc, sA_loc = to_logit(mean_A_loc, std_A_loc)\n",
    "\n",
    "    # 计算系数 k\n",
    "    k = uB_lit / uA_lit if abs(uA_lit) > 1e-8 else 1.0\n",
    "\n",
    "    # 估计 uB_loc\n",
    "    uB_loc = k * uA_loc\n",
    "\n",
    "    # 传播方差\n",
    "    var_k = 0.0\n",
    "    if abs(uA_lit) > 1e-8:\n",
    "        var_k = (sB_lit**2) / (uA_lit**2) + (uB_lit**2) * (sA_lit**2) / (uA_lit**4)\n",
    "    var_uB = (k**2) * (sA_loc**2) + (uA_loc**2) * var_k\n",
    "    sB_loc = math.sqrt(max(var_uB, 0.0))\n",
    "\n",
    "    # 回到原空间\n",
    "    mean_B_loc, std_B_loc = from_logit(uB_loc, sB_loc)\n",
    "    return mean_B_loc, std_B_loc\n",
    "\n",
    "# === 示例 ===\n",
    "# 输入: 模型A文献, 模型B文献, 模型A本地\n",
    "A_lit = (0.9044, 0.001)   # mean, std\n",
    "A_loc = (0.9213, 0.0036)\n",
    "B_lit = (0.8425, 0.007) \n",
    "\n",
    "\n",
    "mean_B_loc, std_B_loc = estimate_B_local(\n",
    "    mean_A_lit=A_lit[0], std_A_lit=A_lit[1],\n",
    "    mean_B_lit=B_lit[0], std_B_lit=B_lit[1],\n",
    "    mean_A_loc=A_loc[0], std_A_loc=A_loc[1]\n",
    ")\n",
    "\n",
    "print(f\"推算本地 B: {mean_B_loc * 100:.2f} ± {std_B_loc * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7373e0-6521-4f3d-8aec-27894de6bd60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# ---------- 基本函数 ----------\n",
    "SQRT2 = math.sqrt(2.0)\n",
    "EPS = 1e-12\n",
    "\n",
    "def phi(x: float) -> float:\n",
    "    \"\"\"标准正态 CDF Φ(x)，用 erf 实现。\"\"\"\n",
    "    return 0.5 * (1.0 + math.erf(x / SQRT2))\n",
    "\n",
    "def inv_phi(y: float) -> float:\n",
    "    \"\"\"Φ^{-1}(y) 的近似（Acklam 近似）。y∈(0,1)\"\"\"\n",
    "    # 参考: https://web.archive.org/web/20150910044702/http://home.online.no/~pjacklam/notes/invnorm/\n",
    "    # 常数\n",
    "    a1,a2,a3,a4,a5,a6 = (-39.6968302866538,220.946098424521,-275.928510446969,138.357751867269,-30.6647980661472,2.50662827745924)\n",
    "    b1,b2,b3,b4,b5 = ( -54.4760987982241,161.585836858041,-155.698979859887,66.8013118877197,-13.2806815528857)\n",
    "    c1,c2,c3,c4,c5,c6 = ( -0.00778489400243029,-0.322396458041136,-2.40075827716184,-2.54973253934373,4.37466414146497,2.93816398269878)\n",
    "    d1,d2,d3,d4 = (0.00778469570904146,0.32246712907004,2.445134137143,3.75440866190742)\n",
    "    p_low  = 0.02425\n",
    "    p_high = 1 - p_low\n",
    "    if y <= 0.0:\n",
    "        return float(\"-inf\")\n",
    "    if y >= 1.0:\n",
    "        return float(\"inf\")\n",
    "    if y < p_low:\n",
    "        q = math.sqrt(-2 * math.log(y))\n",
    "        x = (((((c1*q+c2)*q+c3)*q+c4)*q+c5)*q+c6)/((((d1*q+d2)*q+d3)*q+d4)*q+1)\n",
    "        return -x\n",
    "    elif y > p_high:\n",
    "        q = math.sqrt(-2 * math.log(1 - y))\n",
    "        x = (((((c1*q+c2)*q+c3)*q+c4)*q+c5)*q+c6)/((((d1*q+d2)*q+d3)*q+d4)*q+1)\n",
    "        return x\n",
    "    else:\n",
    "        q = y - 0.5\n",
    "        r = q*q\n",
    "        x = (((((a1*r+a2)*r+a3)*r+a4)*r+a5)*r+a6)*q/(((((b1*r+b2)*r+b3)*r+b4)*r+b5)*r+1)\n",
    "        # 一次牛顿修正提升精度\n",
    "        e = phi(x) - y\n",
    "        x = x - e / (math.exp(-0.5*x*x)/math.sqrt(2*math.pi))\n",
    "        return x\n",
    "\n",
    "def clip01(x: float) -> float:\n",
    "    return min(max(x, EPS), 1.0 - EPS)\n",
    "\n",
    "def logit(p: float) -> float:\n",
    "    p = clip01(p)\n",
    "    return math.log(p/(1-p))\n",
    "\n",
    "def inv_logit(u: float) -> float:\n",
    "    if u >= 0:\n",
    "        z = math.exp(-u)\n",
    "        return 1.0/(1.0+z)\n",
    "    else:\n",
    "        z = math.exp(u)\n",
    "        return z/(1.0+z)\n",
    "\n",
    "# ---------- 二正态模型：由 AUROC 推 d' ----------\n",
    "def auc_to_dprime(auc: float) -> float:\n",
    "    \"\"\"binormal 等方差模型下：AUC = Φ(d'/√2) ⇒ d' = √2 * Φ^{-1}(AUC)\"\"\"\n",
    "    auc = clip01(auc)\n",
    "    return SQRT2 * inv_phi(auc)\n",
    "\n",
    "# ---------- 给定 d' 与 p，计算在阈值 t 下的 TPR/FPR/Precision/Recall/F1 ----------\n",
    "def pr_at_threshold(t: float, dprime: float, p: float) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    设负类 N(0,1)，正类 N(d',1)；阈值 t：score >= t 判为正。\n",
    "    TPR = 1 - Φ((t - d')/1)\n",
    "    FPR = 1 - Φ(t)\n",
    "    Precision = p*TPR / (p*TPR + (1-p)*FPR)\n",
    "    返回 (precision, recall, f1)\n",
    "    \"\"\"\n",
    "    p = clip01(p)\n",
    "    tpr = 1.0 - phi(t - dprime)\n",
    "    fpr = 1.0 - phi(t)\n",
    "    denom = p*tpr + (1.0 - p)*fpr\n",
    "    precision = (p*tpr) / max(denom, EPS)\n",
    "    recall = tpr\n",
    "    if precision + recall <= 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2.0 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def auprc_given_dp_p(dprime: float, p: float, t_min=-8.0, t_max=8.0, steps=4000) -> float:\n",
    "    \"\"\"数值积分阈值以得到 PR 曲线下的面积（梯形法），返回 AUPRC。\"\"\"\n",
    "    p = clip01(p)\n",
    "    # 按阈值从高到低走，可得到 recall 从 0→1 的曲线\n",
    "    dt = (t_max - t_min) / steps\n",
    "    rec_prev, prec_prev = None, None\n",
    "    area = 0.0\n",
    "    for i in range(steps+1):\n",
    "        t = t_max - i*dt\n",
    "        prec, rec, _ = pr_at_threshold(t, dprime, p)\n",
    "        if rec_prev is not None:\n",
    "            # 梯形法：对 recall 积分\n",
    "            drec = rec - rec_prev\n",
    "            area += 0.5 * (prec + prec_prev) * drec\n",
    "        rec_prev, prec_prev = rec, prec\n",
    "    # 数值误差保障\n",
    "    return float(min(max(area, p), 1.0))  # AUPRC >= 基线 p\n",
    "\n",
    "def max_f1_given_dp_p(dprime: float, p: float, t_min=-8.0, t_max=8.0, steps=2000) -> float:\n",
    "    \"\"\"在给定 d' 与 p 下，扫描阈值得到可实现的最大 F1。\"\"\"\n",
    "    best = 0.0\n",
    "    dt = (t_max - t_min) / steps\n",
    "    for i in range(steps+1):\n",
    "        t = t_min + i*dt\n",
    "        _, _, f1 = pr_at_threshold(t, dprime, p)\n",
    "        if f1 > best:\n",
    "            best = f1\n",
    "    return best\n",
    "\n",
    "# ---------- 若未知 p，用 F1 反推（拟合“可实现最大 F1”≈ 观测 F1） ----------\n",
    "def estimate_p_from_f1(dprime: float, f1_target: float, p_lo=1e-4, p_hi=0.99, iters=30) -> float:\n",
    "    \"\"\"\n",
    "    给定 d' 与“观测 F1”（理解为接近最大 F1 的水平），通过 1D 搜索反推 p。\n",
    "    若目标超出可行域，返回最接近的端点。\n",
    "    \"\"\"\n",
    "    f1_target = clip01(f1_target)\n",
    "    # 先检查端点\n",
    "    f1_lo = max_f1_given_dp_p(dprime, p_lo)\n",
    "    f1_hi = max_f1_given_dp_p(dprime, p_hi)\n",
    "    if f1_target <= f1_lo:\n",
    "        return p_lo\n",
    "    if f1_target >= f1_hi:\n",
    "        return p_hi\n",
    "    # 三分/二分混合搜索（对单峰函数鲁棒）\n",
    "    lo, hi = p_lo, p_hi\n",
    "    for _ in range(iters):\n",
    "        m1 = lo + (hi - lo) / 3.0\n",
    "        m2 = hi - (hi - lo) / 3.0\n",
    "        f1_m1 = max_f1_given_dp_p(dprime, m1)\n",
    "        f1_m2 = max_f1_given_dp_p(dprime, m2)\n",
    "        # 以与 f1_target 的距离为损失\n",
    "        if abs(f1_m1 - f1_target) < abs(f1_m2 - f1_target):\n",
    "            hi = m2\n",
    "        else:\n",
    "            lo = m1\n",
    "    p_hat = 0.5 * (lo + hi)\n",
    "    return float(min(max(p_hat, p_lo), p_hi))\n",
    "\n",
    "# ---------- 主函数：由 F1±std 与 AUROC±std 估计 AUPRC±std ----------\n",
    "def estimate_auprc_from_f1_auroc(\n",
    "    f1_mean: float, f1_std: float,\n",
    "    auc_mean: float, auc_std: float,\n",
    "    prevalence: Optional[float] = None,\n",
    "    n_mc: int = 2000,\n",
    "    seed: int = 42\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    依据二正态模型 + 蒙特卡洛，将 (F1±std, AUROC±std) 映射为 AUPRC 的 mean±std。\n",
    "    - 若提供 prevalence（正例率），精度更高；否则用 F1 反推 p。\n",
    "    - 返回: (auprc_mean, auprc_std)\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    auprc_samples = []\n",
    "\n",
    "    for _ in range(n_mc):\n",
    "        # 采样并裁剪到有效区间\n",
    "        auc = clip01(rng.normalvariate(auc_mean, auc_std))\n",
    "        auc = min(max(auc, 0.500001), 1.0 - 1e-6)  # 避免无分辨/越界\n",
    "        f1  = clip01(rng.normalvariate(f1_mean, max(f1_std, 1e-6)))\n",
    "\n",
    "        dprime = auc_to_dprime(auc)\n",
    "\n",
    "        if prevalence is None:\n",
    "            p_hat = estimate_p_from_f1(dprime, f1)\n",
    "        else:\n",
    "            p_hat = clip01(prevalence)\n",
    "\n",
    "            # 若给定 p 下，最大 F1 与观测 F1 相差过大，做一次“温和”的调和校正\n",
    "            f1_max = max_f1_given_dp_p(dprime, p_hat)\n",
    "            # 可选：如果差距极大（>0.1），向能达到观测 F1 的 p 方向微调，但不越过 20%\n",
    "            if abs(f1_max - f1) > 0.10:\n",
    "                p_star = estimate_p_from_f1(dprime, f1)\n",
    "                alpha = 0.2\n",
    "                p_hat = clip01(alpha * p_star + (1 - alpha) * p_hat)\n",
    "\n",
    "        # 数值积分得到 AUPRC\n",
    "        auprc = auprc_given_dp_p(dprime, p_hat)\n",
    "        auprc_samples.append(auprc)\n",
    "\n",
    "    # 统计量\n",
    "    n = len(auprc_samples)\n",
    "    mean = sum(auprc_samples) / n\n",
    "    var  = sum((x - mean) ** 2 for x in auprc_samples) / max(n - 1, 1)\n",
    "    std  = math.sqrt(max(var, 0.0))\n",
    "    return float(mean), float(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7533437a-0feb-445e-af98-3d5aec268255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 情景 2：已知阳性率（强烈推荐，结果更可信）\n",
    "auprc_mean2, auprc_std2 = estimate_auprc_from_f1_auroc(\n",
    "    f1_mean=0.5924, f1_std=0.0046,\n",
    "    auc_mean=0.8625, auc_std=0.0082,\n",
    "    prevalence=0.2685,          # 传入你的测试集正例比例\n",
    "    n_mc=4000,\n",
    "    seed=123\n",
    ")\n",
    "print(f\"AUPRC (given p) ≈ {auprc_mean2 * 100:.2f} ± {auprc_std2 * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c70ae9-5830-4ec3-b8e2-8082641ad55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Iterable, Optional, Tuple\n",
    "\n",
    "def auprc_transfer(auprc_src: float, pi_src: float, pi_tgt: float) -> float:\n",
    "    \"\"\"\n",
    "    Prevalence-corrected AUPRC transfer.\n",
    "    Keeps the \"lift\" over baseline constant across domains.\n",
    "    AUPRC_tgt = pi_tgt + ((AUPRC_src - pi_src) / (1 - pi_src)) * (1 - pi_tgt)\n",
    "    All inputs should be in [0, 1].\n",
    "    \"\"\"\n",
    "    if not (0 <= auprc_src <= 1 and 0 <= pi_src < 1 and 0 <= pi_tgt < 1):\n",
    "        raise ValueError(\"auprc_src, pi_src, pi_tgt must be within [0,1) (AUPRC within [0,1]).\")\n",
    "    if 1 - pi_src <= 0 or 1 - pi_tgt < 0:\n",
    "        raise ValueError(\"Invalid prevalences leading to division by zero.\")\n",
    "    lift = (auprc_src - pi_src) / (1 - pi_src)\n",
    "    return float(pi_tgt + lift * (1 - pi_tgt))\n",
    "\n",
    "\n",
    "def _anchor_shift_stats(\n",
    "    df: pd.DataFrame,\n",
    "    dataset_src: str,\n",
    "    dataset_tgt: str,\n",
    "    task: str,\n",
    "    metric: str,\n",
    "    anchor_models: Optional[Iterable[str]] = None,\n",
    "    use_median: bool = True,\n",
    ") -> Tuple[float, float, int]:\n",
    "    \"\"\"\n",
    "    Compute shift (tgt - src) for a given task & metric using models reported on BOTH datasets.\n",
    "    Returns (shift_central, shift_std, n_anchors).\n",
    "    \"\"\"\n",
    "    sub = df[(df[\"task\"] == task) & (df[\"metric\"] == metric)]\n",
    "    # pivot to wide: index=model, columns=dataset, values=value\n",
    "    wide = sub.pivot_table(index=\"model\", columns=\"dataset\", values=\"value\", aggfunc=\"mean\")\n",
    "    # keep anchors: appear in both datasets\n",
    "    wide = wide.dropna(subset=[dataset_src, dataset_tgt], how=\"any\")\n",
    "    if anchor_models is not None:\n",
    "        wide = wide.loc[wide.index.intersection(set(anchor_models))]\n",
    "    if wide.empty:\n",
    "        raise ValueError(f\"No anchor overlap for task='{task}', metric='{metric}'.\")\n",
    "    deltas = wide[dataset_tgt] - wide[dataset_src]\n",
    "    if use_median:\n",
    "        central = float(deltas.median())\n",
    "    else:\n",
    "        central = float(deltas.mean())\n",
    "    std = float(deltas.std(ddof=1)) if len(deltas) > 1 else 0.0\n",
    "    return central, std, len(deltas)\n",
    "\n",
    "\n",
    "def estimate_missing(\n",
    "    df: pd.DataFrame,\n",
    "    prevalence: Dict[Tuple[str, str], float],\n",
    "    dataset_src: str = \"MIMIC-III\",\n",
    "    dataset_tgt: str = \"MIMIC-IV\",\n",
    "    anchor_models: Optional[Iterable[str]] = None,\n",
    "    use_median_shift: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Estimate missing metrics on dataset_tgt using:\n",
    "      - AUPRC: prevalence-corrected transfer from dataset_src\n",
    "      - AUROC, F1: anchor-based shift learned from models reported on BOTH datasets.\n",
    "\n",
    "    Input DataFrame format (long-form):\n",
    "        columns = [\"model\", \"task\", \"metric\", \"dataset\", \"value\"]\n",
    "        - metric in {\"F1\",\"AUROC\",\"AUPRC\"}\n",
    "        - value in [0,100] or [0,1] (we auto-detect and convert to [0,1] internally)\n",
    "    prevalence: dict mapping (dataset, task) -> positive rate in [0,1]\n",
    "        e.g., {(\"MIMIC-III\",\"Readmission\"):0.4015, (\"MIMIC-IV\",\"Readmission\"):0.7079, ...}\n",
    "\n",
    "    Returns a new DataFrame with original rows plus the estimated rows filled in for\n",
    "    (dataset_tgt, missing cells). Output values are in PERCENT (0-100).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # normalize to [0,1]\n",
    "    needs_scale = df[\"value\"].max() > 1.0 + 1e-8\n",
    "    if needs_scale:\n",
    "        df[\"value\"] = df[\"value\"] / 100.0\n",
    "\n",
    "    tasks = sorted(df[\"task\"].unique())\n",
    "    metrics = [\"F1\", \"AUROC\", \"AUPRC\"]\n",
    "    rows = []\n",
    "\n",
    "    # Pre-compute anchor shifts for F1 & AUROC per task\n",
    "    shift_cache = {}\n",
    "    for t in tasks:\n",
    "        for m in [\"F1\", \"AUROC\"]:\n",
    "            try:\n",
    "                shift_cache[(t, m)] = _anchor_shift_stats(\n",
    "                    df, dataset_src, dataset_tgt, t, m, anchor_models, use_median=use_median_shift\n",
    "                )\n",
    "            except ValueError:\n",
    "                # if no anchors, fall back to zero shift (conservative)\n",
    "                shift_cache[(t, m)] = (0.0, 0.0, 0)\n",
    "\n",
    "    # iterate over source rows and generate target estimates if missing\n",
    "    for (model, task, metric), grp in df.groupby([\"model\", \"task\", \"metric\"]):\n",
    "        # need a src value\n",
    "        src_val_series = grp.loc[grp[\"dataset\"] == dataset_src, \"value\"]\n",
    "        if src_val_series.empty:\n",
    "            continue\n",
    "        src_val = float(src_val_series.iloc[0])\n",
    "\n",
    "        # is target missing?\n",
    "        has_tgt = not grp.loc[grp[\"dataset\"] == dataset_tgt, \"value\"].empty\n",
    "        if has_tgt:\n",
    "            continue\n",
    "\n",
    "        if metric == \"AUPRC\":\n",
    "            pi_src = prevalence[(dataset_src, task)]\n",
    "            pi_tgt = prevalence[(dataset_tgt, task)]\n",
    "            est = auprc_transfer(src_val, pi_src, pi_tgt)\n",
    "            rows.append([model, task, metric, dataset_tgt, est])\n",
    "        elif metric in (\"F1\", \"AUROC\"):\n",
    "            shift, _, _ = shift_cache[(task, metric)]\n",
    "            est = src_val + shift\n",
    "            # clamp to [0,1]\n",
    "            est = min(max(est, 0.0), 1.0)\n",
    "            rows.append([model, task, metric, dataset_tgt, est])\n",
    "\n",
    "    # append estimations\n",
    "    if rows:\n",
    "        add = pd.DataFrame(rows, columns=[\"model\", \"task\", \"metric\", \"dataset\", \"value\"])\n",
    "        df_out = pd.concat([df, add], ignore_index=True)\n",
    "    else:\n",
    "        df_out = df\n",
    "\n",
    "    # return as percentage\n",
    "    df_out[\"value\"] = (df_out[\"value\"] * 100).round(2)\n",
    "    # sort nicely\n",
    "    df_out = df_out.sort_values([\"model\", \"task\", \"metric\", \"dataset\"]).reset_index(drop=True)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def example_usage() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Minimal example to demonstrate the API using a tiny subset from your table.\n",
    "    Values are percentages in the input below (the function will normalize).\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        # model, task, metric, dataset, value(%)\n",
    "        (\"G-BERT\",\"Prolonged Stay\",\"F1\",\"MIMIC-III\",69.62),\n",
    "        (\"G-BERT\",\"Prolonged Stay\",\"AUROC\",\"MIMIC-III\",72.96),\n",
    "        (\"G-BERT\",\"Prolonged Stay\",\"AUPRC\",\"MIMIC-III\",72.48),\n",
    "\n",
    "        (\"StageNet\",\"Prolonged Stay\",\"F1\",\"MIMIC-III\",75.65),\n",
    "        (\"StageNet\",\"Prolonged Stay\",\"AUROC\",\"MIMIC-III\",83.09),\n",
    "        (\"StageNet\",\"Prolonged Stay\",\"AUPRC\",\"MIMIC-III\",84.87),\n",
    "        (\"StageNet\",\"Prolonged Stay\",\"F1\",\"MIMIC-IV\",67.57),\n",
    "        (\"StageNet\",\"Prolonged Stay\",\"AUROC\",\"MIMIC-IV\",84.88),\n",
    "        (\"StageNet\",\"Prolonged Stay\",\"AUPRC\",\"MIMIC-IV\",76.41),\n",
    "\n",
    "        (\"HEART\",\"Prolonged Stay\",\"F1\",\"MIMIC-III\",75.44),\n",
    "        (\"HEART\",\"Prolonged Stay\",\"AUROC\",\"MIMIC-III\",82.99),\n",
    "        (\"HEART\",\"Prolonged Stay\",\"AUPRC\",\"MIMIC-III\",83.83),\n",
    "        (\"HEART\",\"Prolonged Stay\",\"F1\",\"MIMIC-IV\",67.07),\n",
    "        (\"HEART\",\"Prolonged Stay\",\"AUROC\",\"MIMIC-IV\",84.63),\n",
    "        (\"HEART\",\"Prolonged Stay\",\"AUPRC\",\"MIMIC-IV\",74.48),\n",
    "\n",
    "        (\"DT-BEHRT\",\"Prolonged Stay\",\"F1\",\"MIMIC-III\",76.37),\n",
    "        (\"DT-BEHRT\",\"Prolonged Stay\",\"AUROC\",\"MIMIC-III\",84.13),\n",
    "        (\"DT-BEHRT\",\"Prolonged Stay\",\"AUPRC\",\"MIMIC-III\",85.00),\n",
    "        (\"DT-BEHRT\",\"Prolonged Stay\",\"F1\",\"MIMIC-IV\",68.04),\n",
    "        (\"DT-BEHRT\",\"Prolonged Stay\",\"AUROC\",\"MIMIC-IV\",84.98),\n",
    "        (\"DT-BEHRT\",\"Prolonged Stay\",\"AUPRC\",\"MIMIC-IV\",74.78),\n",
    "    ]\n",
    "    df = pd.DataFrame(data, columns=[\"model\",\"task\",\"metric\",\"dataset\",\"value\"])\n",
    "\n",
    "    prevalence = {\n",
    "        (\"MIMIC-III\",\"Prolonged Stay\"): 0.5059,\n",
    "        (\"MIMIC-IV\",\"Prolonged Stay\"): 0.3337,\n",
    "    }\n",
    "\n",
    "    # estimate the missing G-BERT on MIMIC-IV for Prolonged Stay\n",
    "    out = estimate_missing(\n",
    "        df, prevalence,\n",
    "        dataset_src=\"MIMIC-III\", dataset_tgt=\"MIMIC-IV\",\n",
    "        anchor_models=None, use_median_shift=True\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b579d6-cfa2-4769-b121-3a4ba0c321ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9a02f-2481-422c-8edf-9109cd9b9f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fae94-9cfd-481d-be33-56e9a5e2a9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9afbfc-11b9-4b56-8e78-b2c10dcb84a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d07225-ef9d-4c7f-9c81-f4f072bb9d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efeea59-5331-4b91-aa6c-925a793b9dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heart-cuda",
   "language": "python",
   "name": "heart-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
