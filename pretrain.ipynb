{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237db178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from argparse import Namespace\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from heterogt.utils.tokenizer import EHRTokenizer\n",
    "from heterogt.utils.dataset import PreTrainEHRDataset, batcher, expand_level3\n",
    "from heterogt.utils.seed import set_random_seed\n",
    "from heterogt.model.model import HeteroGTPreTrain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c4dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random seed set to 123\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af749748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    dataset = \"MIMIC-III\",\n",
    "    tasks = [\"death\", \"readmission\", \"stay\", \"next_diag_6m\", \"next_diag_12m\"], \n",
    "    task_index = 2,  # index of the task to train\n",
    "    token_type = [\"diag\", \"med\", \"lab\", \"pro\"],\n",
    "    special_tokens = [\"[PAD]\", \"[CLS]\"],\n",
    "    attn_mask_dicts = [{1:[6,7], 2:[6,7], 3:[6,7], 4:[6,7], 6:[2,3,4,5,6,7], 7:[2,3,4,5,6,7]}, \n",
    "                       {1:[6,7], 2:[6,7], 3:[6,7], 4:[6,7], 6:[2,3,4,5,6,7], 7:[2,3,4,5,6,7]}],\n",
    "    d_model = 64,\n",
    "    num_heads = 4,\n",
    "    batch_size = 32,\n",
    "    lr = 1e-3,\n",
    "    epochs = 30,\n",
    "    early_stop_patience = 5,\n",
    "    group_code_thre = 5,  # if there are group_code_thre diag codes belongs to the same group ICD code, then the group code is generated\n",
    "    pretrain_mask_rate = 0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92817ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path = f\"./data_process/{config.dataset}-processed/mimic.pkl\"  # for tokenizer\n",
    "pretrain_data_path = f\"./data_process/{config.dataset}-processed/mimic_pretrain.pkl\" # for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa33dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max admissions per patient: 8\n"
     ]
    }
   ],
   "source": [
    "ehr_full_data = pickle.load(open(full_data_path, 'rb'))\n",
    "group_code_sentences = [expand_level3()[1]]\n",
    "diag_sentences = ehr_full_data[\"ICD9_CODE\"].values.tolist()\n",
    "med_sentences = ehr_full_data[\"NDC\"].values.tolist()\n",
    "lab_sentences = ehr_full_data[\"LAB_TEST\"].values.tolist()\n",
    "pro_sentences = ehr_full_data[\"PRO_CODE\"].values.tolist()\n",
    "age_sentences = [[str(c)] for c in set(ehr_full_data[\"AGE\"].values.tolist())] # important of [[]]\n",
    "max_admissions = ehr_full_data.groupby(\"SUBJECT_ID\")[\"HADM_ID\"].nunique().max()\n",
    "config.max_num_adms = max_admissions\n",
    "print(f\"Max admissions per patient: {config.max_num_adms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7a74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age vocabulary size: 18\n",
      "Group code vocabulary size: 19\n"
     ]
    }
   ],
   "source": [
    "tokenizer = EHRTokenizer(age_sentences, group_code_sentences, diag_sentences, med_sentences, lab_sentences, \n",
    "                         pro_sentences, special_tokens=config.special_tokens)\n",
    "config.label_vocab_size = {\"diag\":len(tokenizer.diag_voc.id2word),     \n",
    "                            \"med\":len(tokenizer.med_voc.id2word), \n",
    "                            \"lab\":len(tokenizer.lab_voc.id2word), \n",
    "                            \"pro\":len(tokenizer.pro_voc.id2word)}\n",
    "config.global_vocab_size = len(tokenizer.vocab.id2word)\n",
    "config.age_vocab_size = tokenizer.token_number(\"age\")\n",
    "config.group_code_vocab_size = tokenizer.token_number(\"group\")\n",
    "print(f\"Age vocabulary size: {config.age_vocab_size}\")\n",
    "print(f\"Group code vocabulary size: {config.group_code_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrain data\n",
    "ehr_pretrain_data = pickle.load(open(pretrain_data_path, 'rb'))\n",
    "# load occurence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pretrain samples: 23146\n"
     ]
    }
   ],
   "source": [
    "pretrain_dataset = PreTrainEHRDataset(ehr_pretrain_data=ehr_pretrain_data, tokenizer=tokenizer, token_type=config.token_type,\n",
    "                                      mask_rate=config.pretrain_mask_rate, group_code_thre=config.group_code_thre, max_num_adms=config.max_num_adms)\n",
    "print(\"Number of pretrain samples:\", len(pretrain_dataset))\n",
    "pretrain_dataloader = DataLoader(pretrain_dataset, batch_size=config.batch_size, \n",
    "                                 collate_fn=batcher(tokenizer, n_token_type=len(config.token_type), is_pretrain=True), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGTPreTrain(tokenizer=tokenizer, token_types=config.token_type, d_model=config.d_model, num_heads=config.num_heads, layer_types=['gnn', 'tf', 'gnn', 'tf'], max_num_adms=config.max_num_adms, \n",
    "                     device=device, label_vocab_size=config.label_vocab_size, attn_mask_dicts=config.attn_mask_dicts,\n",
    "                     use_cls_cat=True).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a9fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001: 100%|██████████| 724/724 [01:36<00:00,  7.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Avg loss: 0.0598 | diag: 0.0265, med: 0.1048, lab: 0.0833, pro: 0.0245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 002:  98%|█████████▊| 711/724 [01:35<00:01,  7.46batch/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "num_visits does not match adm_index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(step_iter):\n\u001b[1;32m      7\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m----> 8\u001b[0m     loss, loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/conda/envs/heart-cuda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/HeteroGT-cuda/src/heterogt/model/model.py:71\u001b[0m, in \u001b[0;36mHeteroGTPreTrain.forward\u001b[0;34m(self, input_ids, token_types, adm_index, age_ids, diag_code_group_dicts, masked_labels)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, token_types, adm_index, age_ids, diag_code_group_dicts, masked_labels):\n\u001b[0;32m---> 71\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madm_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mage_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiag_code_group_dicts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# directly compute loss\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     avg_loss, loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, {}\n",
      "File \u001b[0;32m~/HeteroGT-cuda/src/heterogt/model/model.py:102\u001b[0m, in \u001b[0;36mHeteroGTPreTrain.run_pipeline\u001b[0;34m(self, input_ids, token_types, adm_index, age_ids, diag_code_group_dicts)\u001b[0m\n\u001b[1;32m    100\u001b[0m V \u001b[38;5;241m=\u001b[39m age_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    101\u001b[0m num_visits \u001b[38;5;241m=\u001b[39m (age_ids \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mage_pad_id)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [B], number of visits for each patient in the batch\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(num_visits \u001b[38;5;241m==\u001b[39m (adm_index\u001b[38;5;241m.\u001b[39mmasked_fill(adm_index \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_adms, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_visits does not match adm_index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# 基础表示\u001b[39;00m\n\u001b[1;32m    105\u001b[0m seq_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_emb(input_ids)  \u001b[38;5;66;03m# [B, L, d]\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: num_visits does not match adm_index"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1 + config.epochs):\n",
    "    model.train()\n",
    "    avg_loss, avg_loss_dict = 0., {loss_type: 0. for loss_type in config.token_type}\n",
    "    step_iter = tqdm(pretrain_dataloader, desc=f\"Epoch {epoch:03d}\", unit=\"batch\")\n",
    "    \n",
    "    for step, batch in enumerate(step_iter):\n",
    "        batch = [x.to(device) if isinstance(x, torch.Tensor) else x for x in batch]\n",
    "        loss, loss_dict = model(*batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "        for loss_type in config.token_type:\n",
    "            avg_loss_dict[loss_type] += loss_dict[loss_type]\n",
    "    \n",
    "    avg_loss /= (step + 1)\n",
    "    avg_loss_dict = {loss_type: avg_loss_dict[loss_type] / (step + 1) for loss_type in config.token_type}\n",
    "    loss_str = \", \".join([f\"{t}: {avg_loss_dict[t]:.4f}\" for t in config.token_type])\n",
    "    print(f\"[Epoch {epoch:03d}] Avg loss: {avg_loss:.4f} | {loss_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15c1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model: MIMIC-III-0.7-64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "exp_name = (\n",
    "    f\"{config.dataset}-{config.pretrain_mask_rate}-{config.d_model}\"\n",
    ")\n",
    "save_path = \"./pretrained_models/\" + exp_name\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "torch.save(model.cpu().state_dict(), f\"{save_path}/pretrained_model.pt\")\n",
    "print(\"Save model:\", exp_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
