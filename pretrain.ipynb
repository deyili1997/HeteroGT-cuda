{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237db178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 1.13.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from argparse import Namespace\n",
    "from torch.utils.data import DataLoader\n",
    "from heterogt.utils.tokenizer import EHRTokenizer\n",
    "from heterogt.utils.dataset import PreTrainEHRDataset, batcher, expand_level3\n",
    "from heterogt.utils.seed import set_random_seed\n",
    "from heterogt.model.model import HeteroGTPreTrain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3c4dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random seed set to 123\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af749748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    dataset = \"MIMIC-III\",\n",
    "    tasks = [\"death\", \"readmission\", \"stay\", \"next_diag_6m\", \"next_diag_12m\"], \n",
    "    task_index = 2,  # index of the task to train\n",
    "    token_type = [\"diag\", \"med\", \"lab\", \"pro\"],\n",
    "    special_tokens = [\"[PAD]\", \"[CLS]\"],\n",
    "    attn_mask_dicts = [{1:[6,7], 2:[6,7], 3:[6,7], 4:[6,7], 6:[2,3,4,5,6,7], 7:[2,3,4,5,6,7]}, \n",
    "                       {1:[6,7], 2:[6,7], 3:[6,7], 4:[6,7], 6:[2,3,4,5,6,7], 7:[2,3,4,5,6,7]}],\n",
    "    d_model = 64,\n",
    "    num_heads = 4,\n",
    "    batch_size = 32,\n",
    "    lr = 1e-3,\n",
    "    epochs = 25,\n",
    "    early_stop_patience = 5,\n",
    "    group_code_thre = 5,  # if there are group_code_thre diag codes belongs to the same group ICD code, then the group code is generated\n",
    "    pretrain_mask_rate = 0.7,\n",
    "    cls_ontology_weight = 5e-2,\n",
    "    visit_ontology_weight = 5e-2,\n",
    "    adm_type_weight = 5e-2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92817ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path = f\"./data_process/{config.dataset}-processed/mimic.pkl\"  # for tokenizer\n",
    "pretrain_data_path = f\"./data_process/{config.dataset}-processed/mimic_pretrain.pkl\" # for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95fa33dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max admissions per patient: 8\n"
     ]
    }
   ],
   "source": [
    "ehr_full_data = pickle.load(open(full_data_path, 'rb'))\n",
    "group_code_sentences = [expand_level3()[1]]\n",
    "diag_sentences = ehr_full_data[\"ICD9_CODE\"].values.tolist()\n",
    "med_sentences = ehr_full_data[\"NDC\"].values.tolist()\n",
    "lab_sentences = ehr_full_data[\"LAB_TEST\"].values.tolist()\n",
    "pro_sentences = ehr_full_data[\"PRO_CODE\"].values.tolist()\n",
    "age_sentences = [[str(c)] for c in set(ehr_full_data[\"AGE\"].values.tolist())] # important of [[]]\n",
    "adm_type_sentences = ehr_full_data[\"ADMISSION_TYPE\"].values.tolist()\n",
    "max_admissions = ehr_full_data.groupby(\"SUBJECT_ID\")[\"HADM_ID\"].nunique().max()\n",
    "config.max_num_adms = max_admissions\n",
    "print(f\"Max admissions per patient: {config.max_num_adms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f7a74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age vocabulary size: 18\n",
      "Group code vocabulary size: 19\n"
     ]
    }
   ],
   "source": [
    "tokenizer = EHRTokenizer(age_sentences, group_code_sentences, diag_sentences, med_sentences, lab_sentences, \n",
    "                         pro_sentences, special_tokens=config.special_tokens, adm_types_sentences=adm_type_sentences)\n",
    "config.label_vocab_size = {\"diag\":tokenizer.token_number(\"diag\"),     \n",
    "                            \"med\":tokenizer.token_number(\"med\"), \n",
    "                            \"lab\":tokenizer.token_number(\"lab\"), \n",
    "                            \"pro\":tokenizer.token_number(\"pro\")}\n",
    "config.global_vocab_size = len(tokenizer.vocab.id2word)\n",
    "config.age_vocab_size = tokenizer.token_number(\"age\")\n",
    "config.group_code_vocab_size = tokenizer.token_number(\"group\")\n",
    "print(f\"Age vocabulary size: {config.age_vocab_size}\")\n",
    "print(f\"Group code vocabulary size: {config.group_code_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8248322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrain data\n",
    "ehr_pretrain_data = pickle.load(open(pretrain_data_path, 'rb'))\n",
    "# load occurence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e5676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pretrain samples: 23146\n"
     ]
    }
   ],
   "source": [
    "pretrain_dataset = PreTrainEHRDataset(ehr_pretrain_data=ehr_pretrain_data, tokenizer=tokenizer, token_type=config.token_type,\n",
    "                                      mask_rate=config.pretrain_mask_rate, group_code_thre=config.group_code_thre, max_num_adms=config.max_num_adms)\n",
    "print(\"Number of pretrain samples:\", len(pretrain_dataset))\n",
    "pretrain_dataloader = DataLoader(pretrain_dataset, batch_size=config.batch_size, \n",
    "                                 collate_fn=batcher(tokenizer, n_token_type=len(config.token_type), is_pretrain=True), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac9224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGTPreTrain(tokenizer=tokenizer, token_types=config.token_type, d_model=config.d_model, num_heads=config.num_heads, layer_types=['gnn', 'tf', 'gnn', 'tf'], max_num_adms=config.max_num_adms, \n",
    "                     device=device, label_vocab_size=config.label_vocab_size, attn_mask_dicts=config.attn_mask_dicts,\n",
    "                     use_cls_cat=True).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb79c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diag': 1.0, 'med': 1.0, 'lab': 1.0, 'pro': 1.0, 'cls_ontology': 0.05, 'visit_ontology': 0.05, 'adm_type': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# 统一权重表：未指定的一律 1.0\n",
    "loss_weights = {\n",
    "    **{t: 1.0 for t in config.token_type},\n",
    "    \"cls_ontology\": float(config.cls_ontology_weight),\n",
    "    \"visit_ontology\": float(config.visit_ontology_weight),\n",
    "    \"adm_type\": float(config.adm_type_weight)\n",
    "}\n",
    "loss_types = list(loss_weights.keys())\n",
    "print(loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "809a9fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001:   0%|          | 0/724 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001: 100%|██████████| 724/724 [00:51<00:00, 14.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Weighted total loss: 0.2961\n",
      "  Raw losses      | diag: 0.0264, med: 0.1056, lab: 0.0834, pro: 0.0253, cls_ontology: 0.4266, visit_ontology: 0.4245, adm_type: 0.2573\n",
      "  Contributions   | diag: 0.0264, med: 0.1056, lab: 0.0834, pro: 0.0253, cls_ontology: 0.0213, visit_ontology: 0.0212, adm_type: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 002: 100%|██████████| 724/724 [00:46<00:00, 15.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 002] Weighted total loss: 0.2463\n",
      "  Raw losses      | diag: 0.0175, med: 0.0921, lab: 0.0726, pro: 0.0160, cls_ontology: 0.3856, visit_ontology: 0.3866, adm_type: 0.1909\n",
      "  Contributions   | diag: 0.0175, med: 0.0921, lab: 0.0726, pro: 0.0160, cls_ontology: 0.0193, visit_ontology: 0.0193, adm_type: 0.0095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 003: 100%|██████████| 724/724 [00:48<00:00, 14.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 003] Weighted total loss: 0.2391\n",
      "  Raw losses      | diag: 0.0169, med: 0.0894, lab: 0.0714, pro: 0.0150, cls_ontology: 0.3777, visit_ontology: 0.3806, adm_type: 0.1689\n",
      "  Contributions   | diag: 0.0169, med: 0.0894, lab: 0.0714, pro: 0.0150, cls_ontology: 0.0189, visit_ontology: 0.0190, adm_type: 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 004: 100%|██████████| 724/724 [00:51<00:00, 14.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 004] Weighted total loss: 0.2353\n",
      "  Raw losses      | diag: 0.0167, med: 0.0879, lab: 0.0709, pro: 0.0145, cls_ontology: 0.3716, visit_ontology: 0.3753, adm_type: 0.1588\n",
      "  Contributions   | diag: 0.0167, med: 0.0879, lab: 0.0709, pro: 0.0145, cls_ontology: 0.0186, visit_ontology: 0.0188, adm_type: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 005: 100%|██████████| 724/724 [00:47<00:00, 15.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 005] Weighted total loss: 0.2323\n",
      "  Raw losses      | diag: 0.0165, med: 0.0865, lab: 0.0704, pro: 0.0142, cls_ontology: 0.3677, visit_ontology: 0.3724, adm_type: 0.1532\n",
      "  Contributions   | diag: 0.0165, med: 0.0865, lab: 0.0704, pro: 0.0142, cls_ontology: 0.0184, visit_ontology: 0.0186, adm_type: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 006: 100%|██████████| 724/724 [00:49<00:00, 14.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 006] Weighted total loss: 0.2298\n",
      "  Raw losses      | diag: 0.0163, med: 0.0853, lab: 0.0700, pro: 0.0140, cls_ontology: 0.3648, visit_ontology: 0.3697, adm_type: 0.1493\n",
      "  Contributions   | diag: 0.0163, med: 0.0853, lab: 0.0700, pro: 0.0140, cls_ontology: 0.0182, visit_ontology: 0.0185, adm_type: 0.0075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 007: 100%|██████████| 724/724 [00:48<00:00, 14.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 007] Weighted total loss: 0.2272\n",
      "  Raw losses      | diag: 0.0161, med: 0.0839, lab: 0.0697, pro: 0.0137, cls_ontology: 0.3611, visit_ontology: 0.3666, adm_type: 0.1473\n",
      "  Contributions   | diag: 0.0161, med: 0.0839, lab: 0.0697, pro: 0.0137, cls_ontology: 0.0181, visit_ontology: 0.0183, adm_type: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 008: 100%|██████████| 724/724 [00:44<00:00, 16.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 008] Weighted total loss: 0.2249\n",
      "  Raw losses      | diag: 0.0160, med: 0.0824, lab: 0.0694, pro: 0.0135, cls_ontology: 0.3604, visit_ontology: 0.3658, adm_type: 0.1451\n",
      "  Contributions   | diag: 0.0160, med: 0.0824, lab: 0.0694, pro: 0.0135, cls_ontology: 0.0180, visit_ontology: 0.0183, adm_type: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 009: 100%|██████████| 724/724 [00:51<00:00, 14.07batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 009] Weighted total loss: 0.2231\n",
      "  Raw losses      | diag: 0.0159, med: 0.0814, lab: 0.0691, pro: 0.0134, cls_ontology: 0.3584, visit_ontology: 0.3642, adm_type: 0.1426\n",
      "  Contributions   | diag: 0.0159, med: 0.0814, lab: 0.0691, pro: 0.0134, cls_ontology: 0.0179, visit_ontology: 0.0182, adm_type: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 010: 100%|██████████| 724/724 [00:48<00:00, 14.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 010] Weighted total loss: 0.2214\n",
      "  Raw losses      | diag: 0.0158, med: 0.0804, lab: 0.0689, pro: 0.0132, cls_ontology: 0.3572, visit_ontology: 0.3625, adm_type: 0.1417\n",
      "  Contributions   | diag: 0.0158, med: 0.0804, lab: 0.0689, pro: 0.0132, cls_ontology: 0.0179, visit_ontology: 0.0181, adm_type: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 011: 100%|██████████| 724/724 [00:47<00:00, 15.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 011] Weighted total loss: 0.2201\n",
      "  Raw losses      | diag: 0.0157, med: 0.0799, lab: 0.0686, pro: 0.0130, cls_ontology: 0.3555, visit_ontology: 0.3607, adm_type: 0.1400\n",
      "  Contributions   | diag: 0.0157, med: 0.0799, lab: 0.0686, pro: 0.0130, cls_ontology: 0.0178, visit_ontology: 0.0180, adm_type: 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 012: 100%|██████████| 724/724 [00:47<00:00, 15.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 012] Weighted total loss: 0.2192\n",
      "  Raw losses      | diag: 0.0157, med: 0.0794, lab: 0.0686, pro: 0.0129, cls_ontology: 0.3544, visit_ontology: 0.3595, adm_type: 0.1393\n",
      "  Contributions   | diag: 0.0157, med: 0.0794, lab: 0.0686, pro: 0.0129, cls_ontology: 0.0177, visit_ontology: 0.0180, adm_type: 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 013: 100%|██████████| 724/724 [00:47<00:00, 15.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 013] Weighted total loss: 0.2181\n",
      "  Raw losses      | diag: 0.0156, med: 0.0790, lab: 0.0684, pro: 0.0128, cls_ontology: 0.3541, visit_ontology: 0.3587, adm_type: 0.1365\n",
      "  Contributions   | diag: 0.0156, med: 0.0790, lab: 0.0684, pro: 0.0128, cls_ontology: 0.0177, visit_ontology: 0.0179, adm_type: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 014: 100%|██████████| 724/724 [00:47<00:00, 15.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 014] Weighted total loss: 0.2172\n",
      "  Raw losses      | diag: 0.0155, med: 0.0785, lab: 0.0682, pro: 0.0127, cls_ontology: 0.3525, visit_ontology: 0.3572, adm_type: 0.1361\n",
      "  Contributions   | diag: 0.0155, med: 0.0785, lab: 0.0682, pro: 0.0127, cls_ontology: 0.0176, visit_ontology: 0.0179, adm_type: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 015: 100%|██████████| 724/724 [00:47<00:00, 15.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 015] Weighted total loss: 0.2163\n",
      "  Raw losses      | diag: 0.0155, med: 0.0780, lab: 0.0681, pro: 0.0125, cls_ontology: 0.3527, visit_ontology: 0.3557, adm_type: 0.1365\n",
      "  Contributions   | diag: 0.0155, med: 0.0780, lab: 0.0681, pro: 0.0125, cls_ontology: 0.0176, visit_ontology: 0.0178, adm_type: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 016: 100%|██████████| 724/724 [00:46<00:00, 15.42batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 016] Weighted total loss: 0.2155\n",
      "  Raw losses      | diag: 0.0154, med: 0.0776, lab: 0.0680, pro: 0.0125, cls_ontology: 0.3516, visit_ontology: 0.3557, adm_type: 0.1338\n",
      "  Contributions   | diag: 0.0154, med: 0.0776, lab: 0.0680, pro: 0.0125, cls_ontology: 0.0176, visit_ontology: 0.0178, adm_type: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 017: 100%|██████████| 724/724 [00:46<00:00, 15.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 017] Weighted total loss: 0.2146\n",
      "  Raw losses      | diag: 0.0153, med: 0.0772, lab: 0.0678, pro: 0.0124, cls_ontology: 0.3508, visit_ontology: 0.3535, adm_type: 0.1327\n",
      "  Contributions   | diag: 0.0153, med: 0.0772, lab: 0.0678, pro: 0.0124, cls_ontology: 0.0175, visit_ontology: 0.0177, adm_type: 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 018: 100%|██████████| 724/724 [00:43<00:00, 16.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 018] Weighted total loss: 0.2140\n",
      "  Raw losses      | diag: 0.0153, med: 0.0769, lab: 0.0677, pro: 0.0123, cls_ontology: 0.3507, visit_ontology: 0.3536, adm_type: 0.1315\n",
      "  Contributions   | diag: 0.0153, med: 0.0769, lab: 0.0677, pro: 0.0123, cls_ontology: 0.0175, visit_ontology: 0.0177, adm_type: 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 019: 100%|██████████| 724/724 [00:46<00:00, 15.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 019] Weighted total loss: 0.2134\n",
      "  Raw losses      | diag: 0.0153, med: 0.0766, lab: 0.0677, pro: 0.0123, cls_ontology: 0.3496, visit_ontology: 0.3524, adm_type: 0.1299\n",
      "  Contributions   | diag: 0.0153, med: 0.0766, lab: 0.0677, pro: 0.0123, cls_ontology: 0.0175, visit_ontology: 0.0176, adm_type: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 020: 100%|██████████| 724/724 [00:49<00:00, 14.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 020] Weighted total loss: 0.2127\n",
      "  Raw losses      | diag: 0.0152, med: 0.0762, lab: 0.0676, pro: 0.0122, cls_ontology: 0.3488, visit_ontology: 0.3510, adm_type: 0.1305\n",
      "  Contributions   | diag: 0.0152, med: 0.0762, lab: 0.0676, pro: 0.0122, cls_ontology: 0.0174, visit_ontology: 0.0176, adm_type: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 021: 100%|██████████| 724/724 [00:49<00:00, 14.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 021] Weighted total loss: 0.2122\n",
      "  Raw losses      | diag: 0.0151, med: 0.0760, lab: 0.0674, pro: 0.0122, cls_ontology: 0.3478, visit_ontology: 0.3508, adm_type: 0.1310\n",
      "  Contributions   | diag: 0.0151, med: 0.0760, lab: 0.0674, pro: 0.0122, cls_ontology: 0.0174, visit_ontology: 0.0175, adm_type: 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 022: 100%|██████████| 724/724 [00:48<00:00, 15.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 022] Weighted total loss: 0.2116\n",
      "  Raw losses      | diag: 0.0151, med: 0.0757, lab: 0.0674, pro: 0.0121, cls_ontology: 0.3474, visit_ontology: 0.3498, adm_type: 0.1296\n",
      "  Contributions   | diag: 0.0151, med: 0.0757, lab: 0.0674, pro: 0.0121, cls_ontology: 0.0174, visit_ontology: 0.0175, adm_type: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 023: 100%|██████████| 724/724 [00:46<00:00, 15.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 023] Weighted total loss: 0.2112\n",
      "  Raw losses      | diag: 0.0151, med: 0.0757, lab: 0.0673, pro: 0.0120, cls_ontology: 0.3467, visit_ontology: 0.3485, adm_type: 0.1287\n",
      "  Contributions   | diag: 0.0151, med: 0.0757, lab: 0.0673, pro: 0.0120, cls_ontology: 0.0173, visit_ontology: 0.0174, adm_type: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 024: 100%|██████████| 724/724 [00:46<00:00, 15.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 024] Weighted total loss: 0.2106\n",
      "  Raw losses      | diag: 0.0150, med: 0.0754, lab: 0.0672, pro: 0.0119, cls_ontology: 0.3459, visit_ontology: 0.3480, adm_type: 0.1272\n",
      "  Contributions   | diag: 0.0150, med: 0.0754, lab: 0.0672, pro: 0.0119, cls_ontology: 0.0173, visit_ontology: 0.0174, adm_type: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 025: 100%|██████████| 724/724 [00:50<00:00, 14.37batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 025] Weighted total loss: 0.2104\n",
      "  Raw losses      | diag: 0.0150, med: 0.0752, lab: 0.0671, pro: 0.0119, cls_ontology: 0.3463, visit_ontology: 0.3484, adm_type: 0.1290\n",
      "  Contributions   | diag: 0.0150, med: 0.0752, lab: 0.0671, pro: 0.0119, cls_ontology: 0.0173, visit_ontology: 0.0174, adm_type: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1 + config.epochs):\n",
    "    model.train()\n",
    "    avg_total_loss = 0.0\n",
    "    avg_loss_dict = {t: 0.0 for t in loss_types}         # 未加权分项\n",
    "    avg_contrib_dict = {t: 0.0 for t in loss_types}      # 加权后贡献\n",
    "    step_iter = tqdm(pretrain_dataloader, desc=f\"Epoch {epoch:03d}\", unit=\"batch\")\n",
    "\n",
    "    for step, batch in enumerate(step_iter):\n",
    "        batch = [x.to(device) if isinstance(x, torch.Tensor) else x for x in batch]\n",
    "        loss_dict = model(*batch)   # {loss_type: tensor}\n",
    "\n",
    "        # 总 loss（加权）\n",
    "        total_loss = sum(loss_weights[t] * loss_dict[t] for t in loss_types)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累积\n",
    "        avg_total_loss += total_loss.item()\n",
    "        for t in loss_types:\n",
    "            avg_loss_dict[t] += loss_dict[t].item()                       # 原始 loss\n",
    "            avg_contrib_dict[t] += (loss_weights[t] * loss_dict[t]).item()  # 加权后贡献\n",
    "\n",
    "    steps = step + 1\n",
    "    avg_total_loss /= steps\n",
    "    avg_loss_dict = {t: v / steps for t, v in avg_loss_dict.items()}\n",
    "    avg_contrib_dict = {t: v / steps for t, v in avg_contrib_dict.items()}\n",
    "\n",
    "    # 日志字符串\n",
    "    raw_loss_str = \", \".join(f\"{t}: {avg_loss_dict[t]:.4f}\" for t in loss_types)\n",
    "    contrib_str  = \", \".join(f\"{t}: {avg_contrib_dict[t]:.4f}\" for t in loss_types)\n",
    "\n",
    "    print(f\"[Epoch {epoch:03d}] Weighted total loss: {avg_total_loss:.4f}\\n\"\n",
    "          f\"  Raw losses      | {raw_loss_str}\\n\"\n",
    "          f\"  Contributions   | {contrib_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a15c1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model: MIMIC-III-0.7-64-0.05-0.05-0.05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "exp_name = (\n",
    "    f\"{config.dataset}-{config.pretrain_mask_rate}-{config.d_model}-{config.cls_ontology_weight}-{config.visit_ontology_weight}-{config.adm_type_weight}\"\n",
    ")\n",
    "save_path = \"./pretrained_models/\" + exp_name\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "torch.save(model.cpu().state_dict(), f\"{save_path}/pretrained_model.pt\")\n",
    "print(\"Save model:\", exp_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
